{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import sin, cos\n",
    "import numba as nb\n",
    "from scipy.optimize import fsolve\n",
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# power flow function (equality constraints)\n",
    "def gfun(x, u, p):\n",
    "  \n",
    "    VM3 = x[0]\n",
    "    VA3 = x[1]\n",
    "    VA2 = x[2]\n",
    "    \n",
    "    VM1 = u[0]\n",
    "    P2 = u[1]\n",
    "    VM2 = u[2]\n",
    "    \n",
    "    VA1 = p[0]\n",
    "    P3 = p[1]\n",
    "    Q3 = p[2]\n",
    "\n",
    "    # intermediate quantities\n",
    "    VA23 = VA2 - VA3\n",
    "    VA31 = VA3 - VA1\n",
    "    VA32 = VA3 - VA2\n",
    "    \n",
    "    F1 = 4.0*VM2*VM2 + VM2*VM3*(-4*cos(VA23) + 10*sin(VA23)) - P2\n",
    "    F2 = (8.0*VM3*VM3 + VM3*VM1*(-4*cos(VA31) + 5*sin(VA31))\n",
    "          + VM3*VM2*(-4*cos(VA32) + 10*sin(VA32)) + P3)\n",
    "    F3 = (15.0*VM3*VM3 + VM3*VM1*(-4*sin(VA31) - 5*cos(VA31))\n",
    "          + VM3*VM2*(-4*sin(VA32) - 10*cos(VA32)) + Q3)\n",
    "\n",
    "    return np.array([F1, F2, F3])\n",
    "\n",
    "# cost function\n",
    "def cfun(x, u, p):\n",
    "\n",
    "    VM3 = x[0]\n",
    "    VA3 = x[1]\n",
    "\n",
    "    VM1 = u[0]\n",
    "    P2 = u[1]\n",
    "\n",
    "    VA1 = p[0]\n",
    "\n",
    "    VA13 = VA1 - VA3\n",
    "    \n",
    "    w1 = 1.0\n",
    "    w2 = 1.0\n",
    "\n",
    "    cost = (w1*(4.0*VM1*VM1 + VM1*VM3*(-4*cos(VA13) + 5*sin(VA13))) +\n",
    "          w2*P2)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobians and gradients\n",
    "\n",
    "def gfun_x(x, u, p):\n",
    "\n",
    "    \n",
    "    VM3 = x[0]\n",
    "    VA3 = x[1]\n",
    "    VA2 = x[2]\n",
    "    \n",
    "    VM1 = u[0]\n",
    "    P2 = u[1]\n",
    "    VM2 = u[2]\n",
    "    \n",
    "    VA1 = p[0]\n",
    "    P3 = p[1]\n",
    "    Q3 = p[2]\n",
    "\n",
    "    # intermediate quantities\n",
    "    VA23 = VA2 - VA3\n",
    "    VA31 = VA3 - VA1\n",
    "    VA32 = VA3 - VA2\n",
    "    \n",
    "    J = np.zeros((3, 3))\n",
    "    \n",
    "    #F1\n",
    "    J[0, 0] =  VM2*(10*sin(VA2 - VA3) - 4*cos(VA2 - VA3))\n",
    "    J[0, 1] =  VM2*VM3*(-4*sin(VA2 - VA3) - 10*cos(VA2 - VA3))\n",
    "    J[0, 2] =  VM2*VM3*(4*sin(VA2 - VA3) + 10*cos(VA2 - VA3))\n",
    "    #F2\n",
    "    J[1, 0] =  VM1*(-5*sin(VA1 - VA3) - 4*cos(VA1 - VA3)) + VM2*(-10*sin(VA2 - VA3) - 4*cos(VA2 - VA3)) + 16.0*VM3\n",
    "    J[1, 1] =  VM1*VM3*(-4*sin(VA1 - VA3) + 5*cos(VA1 - VA3)) + VM2*VM3*(-4*sin(VA2 - VA3) + 10*cos(VA2 - VA3))\n",
    "    J[1, 2] =  VM2*VM3*(4*sin(VA2 - VA3) - 10*cos(VA2 - VA3))\n",
    "    #F3\n",
    "    J[2, 0] =  VM1*(4*sin(VA1 - VA3) - 5*cos(VA1 - VA3)) + VM2*(4*sin(VA2 - VA3) - 10*cos(VA2 - VA3)) + 30.0*VM3\n",
    "    J[2, 1] =  VM1*VM3*(-5*sin(VA1 - VA3) - 4*cos(VA1 - VA3)) + VM2*VM3*(-10*sin(VA2 - VA3) - 4*cos(VA2 - VA3))\n",
    "    J[2, 2] =  VM2*VM3*(10*sin(VA2 - VA3) + 4*cos(VA2 - VA3))\n",
    "\n",
    "    return J\n",
    "\n",
    "def gfun_u(x, u, p):\n",
    "    \n",
    "    VM3 = x[0]\n",
    "    VA3 = x[1]\n",
    "    VA2 = x[2]\n",
    "    \n",
    "    VM1 = u[0]\n",
    "    P2 = u[1]\n",
    "    VM2 = u[2]\n",
    "    \n",
    "    VA1 = p[0]\n",
    "    P3 = p[1]\n",
    "    Q3 = p[2]\n",
    "\n",
    "    # intermediate quantities\n",
    "    VA23 = VA2 - VA3\n",
    "    VA31 = VA3 - VA1\n",
    "    VA32 = VA3 - VA2\n",
    "    \n",
    "    J = np.zeros((3, 3))\n",
    "    \n",
    "    #F1\n",
    "    J[0, 0] =  0\n",
    "    J[0, 1] =  -1\n",
    "    J[0, 2] =  8.0*VM2 + VM3*(10*sin(VA2 - VA3) - 4*cos(VA2 - VA3))\n",
    "    #F2\n",
    "    J[1, 0] =  VM3*(-5*sin(VA1 - VA3) - 4*cos(VA1 - VA3))\n",
    "    J[1, 1] =  0\n",
    "    J[1, 2] =  VM3*(-10*sin(VA2 - VA3) - 4*cos(VA2 - VA3))\n",
    "    #F3\n",
    "    J[1, 0] =  VM3*(-5*sin(VA1 - VA3) - 4*cos(VA1 - VA3))\n",
    "    J[1, 1] =  0\n",
    "    J[1, 2] =  VM3*(-10*sin(VA2 - VA3) - 4*cos(VA2 - VA3))\n",
    "\n",
    "    return J\n",
    "\n",
    "\n",
    "def cfun_x(x, u, p):\n",
    "    \n",
    "    VM3 = x[0]\n",
    "    VA3 = x[1]\n",
    "\n",
    "    VM1 = u[0]\n",
    "    P2 = u[1]\n",
    "\n",
    "    VA1 = p[0]\n",
    "\n",
    "    VA13 = VA1 - VA3\n",
    "    \n",
    "    w1 = 1.0\n",
    "    w2 = 1.0\n",
    "    \n",
    "    grad = np.zeros(3)\n",
    "    grad[0] =  VM1*w1*(5*sin(VA1 - VA3) - 4*cos(VA1 - VA3))\n",
    "    grad[1] =  VM1*VM3*w1*(-4*sin(VA1 - VA3) - 5*cos(VA1 - VA3))\n",
    "    grad[2] =  0\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def cfun_u(x, u, p):\n",
    "    \n",
    "    VM3 = x[0]\n",
    "    VA3 = x[1]\n",
    "\n",
    "    VM1 = u[0]\n",
    "    P2 = u[1]\n",
    "\n",
    "    VA1 = p[0]\n",
    "\n",
    "    VA13 = VA1 - VA3\n",
    "    \n",
    "    w1 = 1.0\n",
    "    w2 = 1.0\n",
    "    \n",
    "    grad = np.zeros(3)\n",
    "    grad[0] =  w1*(8.0*VM1 + VM3*(5*sin(VA1 - VA3) - 4*cos(VA1 - VA3)))\n",
    "    grad[1] =  w2\n",
    "    grad[2] =  0\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize script with same initial conditions as in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n",
      "[1.  1.7 1. ]\n"
     ]
    }
   ],
   "source": [
    "# initial parameters\n",
    "x = np.zeros(3)\n",
    "u = np.zeros(3)\n",
    "p = np.zeros(3)\n",
    "\n",
    "# this is an initial guess\n",
    "x[0] = 1.0 #VM3\n",
    "x[1] = 0.0 #VA3\n",
    "x[2] = 0.0 #VA2\n",
    "\n",
    "# this is given by the problem data, but might be \"controlled\" via OPF\n",
    "u[0] = 1.0 #VM1\n",
    "u[1] = 1.7 #P2\n",
    "u[2] = 1.0 #VM2\n",
    "\n",
    "# these parameters are fixed through the computation\n",
    "p[0] = 0.0 #VA1, slack angle\n",
    "p[1] = 2.0 #P3\n",
    "p[2] = 1.0 #Q3\n",
    "\n",
    "# print initial guesses\n",
    "print(x)\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.88186783 -0.00094814  0.1349708 ]\n"
     ]
    }
   ],
   "source": [
    "# POWER FLOW ALGO\n",
    "\n",
    "def powerflow(x, u, p):\n",
    "    \n",
    "    sol = fsolve(gfun, x, args=(u,p,))\n",
    "    return sol\n",
    "    \n",
    "print(powerflow(x, u, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of gradient:  0.6806350735577951\n",
      "Cost function:  2.176710951198614\n",
      "Norm of gradient:  0.5208553342382815\n",
      "Cost function:  2.1795465377422643\n",
      "Norm of gradient:  0.5301385380257911\n",
      "Cost function:  2.195685078446081\n",
      "Norm of gradient:  0.5310377877882734\n",
      "Cost function:  2.2141979269924628\n",
      "Norm of gradient:  0.5270877786707068\n",
      "Cost function:  2.236391035785762\n",
      "Norm of gradient:  0.5135887282073747\n",
      "Cost function:  2.263130479743788\n",
      "Norm of gradient:  0.4842284003188323\n",
      "Cost function:  2.295526425593832\n",
      "Norm of gradient:  0.42879188346856134\n",
      "Cost function:  2.3344751241358668\n",
      "Norm of gradient:  0.3348081832575279\n",
      "Cost function:  2.3794545825224938\n",
      "Norm of gradient:  0.20308910140753886\n",
      "Cost function:  2.4253478730492573\n",
      "Norm of gradient:  0.09068333078146222\n",
      "Cost function:  2.4592807927826703\n",
      "Norm of gradient:  0.06752092366428539\n",
      "Cost function:  2.4713977136177006\n",
      "Norm of gradient:  0.06492193323234306\n",
      "Cost function:  2.4720426678475347\n",
      "Norm of gradient:  0.06246097226096025\n",
      "Cost function:  2.471798867718383\n",
      "Norm of gradient:  0.059772592096422925\n",
      "Cost function:  2.4713326211784983\n",
      "Norm of gradient:  0.05751224180461684\n",
      "Cost function:  2.4711148295762353\n",
      "Norm of gradient:  0.05503956440584259\n",
      "Cost function:  2.4707010420497006\n",
      "Norm of gradient:  0.052967298344454294\n",
      "Cost function:  2.470521003716704\n",
      "Norm of gradient:  0.05069327852921905\n",
      "Cost function:  2.4701538401552945\n",
      "Norm of gradient:  0.04879145785372926\n",
      "Cost function:  2.4700047473444013\n",
      "Norm of gradient:  0.04670031601036594\n",
      "Cost function:  2.4696789049943453\n",
      "Norm of gradient:  0.04495322317665153\n",
      "Cost function:  2.46955511617137\n",
      "Norm of gradient:  0.04303042665552645\n",
      "Cost function:  2.4692659480790864\n",
      "Norm of gradient:  0.04142403948223428\n",
      "Cost function:  2.469162810799591\n",
      "Norm of gradient:  0.039656185829087476\n",
      "Cost function:  2.4689062217644464\n",
      "Norm of gradient:  0.03817797401064772\n",
      "Cost function:  2.4688199152175385\n",
      "Norm of gradient:  0.03655269367103218\n",
      "Cost function:  2.468592280238857\n",
      "Norm of gradient:  0.035191438171852475\n",
      "Cost function:  2.46851967904388\n",
      "Norm of gradient:  0.033697311717900885\n",
      "Cost function:  2.4683177787296984\n",
      "Norm of gradient:  0.032442945627529605\n",
      "Cost function:  2.468256335918662\n",
      "Norm of gradient:  0.031069431738604922\n",
      "Cost function:  2.4680773045773603\n",
      "Norm of gradient:  0.029912900854301924\n",
      "Cost function:  2.4680249517533133\n",
      "Norm of gradient:  0.028650272277160042\n",
      "Cost function:  2.467866234950576\n",
      "Norm of gradient:  0.027583413521472894\n",
      "Cost function:  2.4678212976803557\n",
      "Norm of gradient:  0.026422699025277007\n",
      "Cost function:  2.4676806168785514\n",
      "Norm of gradient:  0.025438134769555965\n",
      "Cost function:  2.4676417434693523\n",
      "Norm of gradient:  0.024371065718789775\n",
      "Cost function:  2.4675170660088823\n",
      "Norm of gradient:  0.023462112135357985\n",
      "Cost function:  2.4674831679326283\n",
      "Norm of gradient:  0.022481072756460704\n",
      "Cost function:  2.467372681099513\n",
      "Norm of gradient:  0.02164166043287742\n",
      "Cost function:  2.467342883463078\n",
      "Norm of gradient:  0.020739641176176528\n",
      "Cost function:  2.4672449717457705\n",
      "Norm of gradient:  0.019964246370413906\n",
      "Cost function:  2.4672185723516886\n",
      "Norm of gradient:  0.019134799994322306\n",
      "Cost function:  2.4671317972505316\n",
      "Norm of gradient:  0.018418385071663355\n",
      "Cost function:  2.4671082329450664\n",
      "Norm of gradient:  0.017655585224890673\n",
      "Cost function:  2.467031314883509\n",
      "Norm of gradient:  0.016993546982901446\n",
      "Cost function:  2.4670101340404473\n",
      "Norm of gradient:  0.01629194915264779\n",
      "Cost function:  2.466941936056913\n",
      "Norm of gradient:  0.015680073901153974\n",
      "Cost function:  2.4669227761914225\n",
      "Norm of gradient:  0.01503467864738304\n",
      "Cost function:  2.4668622891797085\n",
      "Norm of gradient:  0.014469103060626808\n",
      "Cost function:  2.4668448588235057\n",
      "Norm of gradient:  0.013875321481325243\n",
      "Cost function:  2.466791188149313\n",
      "Norm of gradient:  0.013352498376443492\n",
      "Cost function:  2.4667752522438278\n",
      "Norm of gradient:  0.012806119756154026\n",
      "Cost function:  2.466727605604432\n",
      "Norm of gradient:  0.012322788074639147\n",
      "Cost function:  2.4667129737816103\n",
      "Norm of gradient:  0.011819949665505711\n",
      "Cost function:  2.466670650201091\n",
      "Norm of gradient:  0.011373108042227959\n",
      "Cost function:  2.4666571674215287\n",
      "Norm of gradient:  0.010910266918383696\n",
      "Cost function:  2.4666195472901156\n",
      "Norm of gradient:  0.010497150316709867\n",
      "Cost function:  2.46660708639566\n",
      "Norm of gradient:  0.01007105723251089\n",
      "Cost function:  2.4665736224721213\n",
      "Norm of gradient:  0.009689116205018018\n",
      "Cost function:  2.4665620782855333\n",
      "Norm of gradient:  0.009296791377429548\n",
      "Cost function:  2.466532287588185\n",
      "Norm of gradient:  0.008943673580970833\n",
      "Cost function:  2.4665215722570126\n",
      "Norm of gradient:  0.008582384307629129\n",
      "Cost function:  2.4664950287735836\n",
      "Norm of gradient:  0.008255917960334529\n",
      "Cost function:  2.46648506811004\n",
      "Norm of gradient:  0.007923157978092597\n",
      "Cost function:  2.4664613962601187\n",
      "Norm of gradient:  0.0076213369955582195\n",
      "Cost function:  2.4664521268746475\n",
      "Norm of gradient:  0.0073148074798291595\n",
      "Cost function:  2.4664309956616934\n",
      "Norm of gradient:  0.007035778069500819\n",
      "Cost function:  2.466422362726103\n",
      "Norm of gradient:  0.006753370172391132\n",
      "Cost function:  2.466403480518847\n",
      "Norm of gradient:  0.006495418700149418\n",
      "Cost function:  2.4663954360267057\n",
      "Norm of gradient:  0.006235197524998918\n",
      "Cost function:  2.4663785459129026\n",
      "Norm of gradient:  0.005996739497080771\n",
      "Cost function:  2.466371047330816\n",
      "Norm of gradient:  0.00575692940836633\n",
      "Cost function:  2.466355922989565\n",
      "Norm of gradient:  0.005536499436089883\n",
      "Cost function:  2.4663489322144008\n",
      "Norm of gradient:  0.005315470606245267\n",
      "Cost function:  2.4663353742564644\n",
      "Norm of gradient:  0.005111713241160989\n",
      "Cost function:  2.466328856810886\n",
      "Norm of gradient:  0.0049079693395894086\n",
      "Cost function:  2.466316689540003\n",
      "Norm of gradient:  0.004719630683420998\n",
      "Cost function:  2.466310613952767\n",
      "Norm of gradient:  0.004531797617386261\n",
      "Cost function:  2.466299682504347\n",
      "Norm of gradient:  0.004357717625015406\n",
      "Cost function:  2.4662940198331276\n",
      "Norm of gradient:  0.004184533247165612\n",
      "Cost function:  2.466284187650651\n",
      "Norm of gradient:  0.004023638652415504\n",
      "Cost function:  2.4662789111138412\n",
      "Norm of gradient:  0.0038639433538636774\n",
      "Cost function:  2.466270057723965\n",
      "Norm of gradient:  0.0037152411578762166\n",
      "Cost function:  2.4662651424183935\n",
      "Norm of gradient:  0.003567969274379518\n",
      "Cost function:  2.4662571614757947\n",
      "Norm of gradient:  0.0034305407418109128\n",
      "Cost function:  2.4662525841526444\n",
      "Norm of gradient:  0.0032947127011319984\n",
      "Cost function:  2.4662453817215226\n",
      "Norm of gradient:  0.0031677078220773754\n",
      "Cost function:  2.4662411206157193\n",
      "Norm of gradient:  0.003042422968537639\n",
      "Cost function:  2.4662346136580453\n",
      "Norm of gradient:  0.002925055341657532\n",
      "Cost function:  2.4662306483507805\n",
      "Norm of gradient:  0.0028094853805756955\n",
      "Cost function:  2.466224763402189\n",
      "Norm of gradient:  0.002701027483480518\n",
      "Cost function:  2.466221074710635\n",
      "Norm of gradient:  0.002594410489626126\n",
      "Cost function:  2.4662157467186754\n",
      "Norm of gradient:  0.0024941893042598477\n",
      "Cost function:  2.4662123166056955\n",
      "Norm of gradient:  0.0023958242449405665\n",
      "Cost function:  2.4662074879116\n",
      "Norm of gradient:  0.0023032172093389266\n",
      "Cost function:  2.4662042994102036\n",
      "Norm of gradient:  0.0022124589366123047\n",
      "Cost function:  2.466199918856767\n",
      "Norm of gradient:  0.0021268901972280652\n",
      "Cost function:  2.4661969560052865\n",
      "Norm of gradient:  0.0020431448677129685\n",
      "Cost function:  2.466192978155497\n",
      "Norm of gradient:  0.0019640818090837456\n",
      "Cost function:  2.4661902259407613\n",
      "Norm of gradient:  0.0018868026934592142\n",
      "Cost function:  2.4661866103934234\n",
      "Norm of gradient:  0.0018137527240018954\n",
      "Cost function:  2.4661840546997147\n",
      "Norm of gradient:  0.0017424363717406367\n",
      "Cost function:  2.4661807654901162\n",
      "Norm of gradient:  0.0016749439463983008\n",
      "Cost function:  2.4661783930524503\n",
      "Norm of gradient:  0.0016091266743496983\n",
      "Cost function:  2.4661753981274313\n",
      "Norm of gradient:  0.001546770536406064\n",
      "Cost function:  2.4661731964880422\n",
      "Norm of gradient:  0.0014860252126701614\n",
      "Cost function:  2.4661704672461306\n",
      "Norm of gradient:  0.0014284158385392185\n",
      "Cost function:  2.4661684247134543\n",
      "Norm of gradient:  0.0013723489357098432\n",
      "Cost function:  2.4661659356018957\n",
      "Norm of gradient:  0.0013191261677549845\n",
      "Cost function:  2.466164041211491\n",
      "Norm of gradient:  0.0012673750619376584\n",
      "Cost function:  2.466161769372982\n",
      "Norm of gradient:  0.0012182059155793972\n",
      "Cost function:  2.4661600128501022\n",
      "Norm of gradient:  0.0011704364097694664\n",
      "Cost function:  2.4661579378129574\n",
      "Norm of gradient:  0.0011250130421510114\n",
      "Cost function:  2.466156309536505\n",
      "Norm of gradient:  0.0010809170944878104\n",
      "Cost function:  2.4661544129427746\n",
      "Norm of gradient:  0.0010389549229541835\n",
      "Cost function:  2.466152903910534\n",
      "Norm of gradient:  0.0009982485621361943\n",
      "Cost function:  2.466151169277234\n",
      "Norm of gradient:  0.0009594845216376059\n",
      "Cost function:  2.4661497710723332\n",
      "Norm of gradient:  0.000921905933419299\n",
      "Cost function:  2.4661481835816295\n",
      "Norm of gradient:  0.0008860968627397059\n",
      "Cost function:  2.466146888340169\n",
      "Norm of gradient:  0.0008514046328400697\n",
      "Cost function:  2.466145434654827\n",
      "Norm of gradient:  0.0008183257802920305\n",
      "Cost function:  2.466144235034677\n",
      "Norm of gradient:  0.0007862972804236441\n",
      "Cost function:  2.466142903135628\n",
      "Norm of gradient:  0.0007557409203200298\n",
      "Cost function:  2.4661417922864026\n",
      "Norm of gradient:  0.0007261708251808894\n",
      "Cost function:  2.4661405713296123\n",
      "Norm of gradient:  0.0006979449770253782\n",
      "Cost function:  2.4661395428638224\n",
      "Norm of gradient:  0.0006706439012090445\n",
      "Cost function:  2.4661384230540246\n",
      "Norm of gradient:  0.0006445711441138353\n",
      "Cost function:  2.4661374710194286\n",
      "Norm of gradient:  0.0006193643888965817\n",
      "Cost function:  2.4661364434986757\n",
      "Norm of gradient:  0.0005952807642731688\n",
      "Cost function:  2.466135562351827\n",
      "Norm of gradient:  0.0005720071650708615\n",
      "Cost function:  2.4661346191009437\n",
      "Norm of gradient:  0.0005497611611363206\n",
      "Cost function:  2.4661338036819234\n",
      "Norm of gradient:  0.0005282720272734104\n",
      "Cost function:  2.4661329374333114\n",
      "Norm of gradient:  0.0005077236393895608\n",
      "Cost function:  2.466132182941684\n",
      "Norm of gradient:  0.00048788177854194496\n",
      "Cost function:  2.4661313871020534\n",
      "Norm of gradient:  0.00046890163979871534\n",
      "Cost function:  2.466130689073975\n",
      "Norm of gradient:  0.0004505804601256699\n",
      "Cost function:  2.4661299576558062\n",
      "Norm of gradient:  0.00043304903702101816\n",
      "Cost function:  2.466129311942322\n",
      "Norm of gradient:  0.00041613172061391915\n",
      "Cost function:  2.4661286395029665\n",
      "Norm of gradient:  0.0003999385690254764\n",
      "Cost function:  2.4661280422494585\n",
      "Norm of gradient:  0.00038431731084952685\n",
      "Cost function:  2.4661274238369852\n",
      "Norm of gradient:  0.00036936038783538675\n",
      "Cost function:  2.466126871463718\n",
      "Norm of gradient:  0.000354935694829092\n",
      "Cost function:  2.4661263025686813\n",
      "Norm of gradient:  0.0003411207221430084\n",
      "Cost function:  2.466125791752457\n",
      "Norm of gradient:  0.0003278007675995833\n",
      "Cost function:  2.4661252682648804\n",
      "Norm of gradient:  0.00031504064307850857\n",
      "Cost function:  2.466124795921739\n",
      "Norm of gradient:  0.0003027406718404164\n",
      "Cost function:  2.4661243140927045\n",
      "Norm of gradient:  0.0002909549251069693\n",
      "Cost function:  2.4661238773616354\n",
      "Norm of gradient:  0.0002795967054836649\n",
      "Cost function:  2.466123433768937\n",
      "Norm of gradient:  0.00026871099467953407\n",
      "Cost function:  2.4661230299965933\n",
      "Norm of gradient:  0.0002582223133401054\n",
      "Cost function:  2.4661226215139767\n",
      "Norm of gradient:  0.0002481679598099083\n",
      "Cost function:  2.466122248240296\n",
      "Norm of gradient:  0.00023848215620976058\n",
      "Cost function:  2.46612187200988\n",
      "Norm of gradient:  0.00022919571433079855\n",
      "Cost function:  2.4661215269546286\n",
      "Norm of gradient:  0.00022025125152307873\n",
      "Cost function:  2.4661211803621628\n",
      "Norm of gradient:  0.00021167411102836667\n",
      "Cost function:  2.4661208614123016\n",
      "Norm of gradient:  0.00020341417995944948\n",
      "Cost function:  2.466120542064904\n",
      "Norm of gradient:  0.00019549219834447826\n",
      "Cost function:  2.4661202472628028\n",
      "Norm of gradient:  0.0001878643529801319\n",
      "Cost function:  2.466119952968933\n",
      "Norm of gradient:  0.00018054751572078135\n",
      "Cost function:  2.4661196805013215\n",
      "Norm of gradient:  0.00017350333656426936\n",
      "Cost function:  2.4661194092527214\n",
      "Norm of gradient:  0.00016674544306842664\n",
      "Cost function:  2.4661191574403976\n",
      "Norm of gradient:  0.00016024022682897498\n",
      "Cost function:  2.4661189073958125\n",
      "Norm of gradient:  0.00015399860018419257\n",
      "Cost function:  2.4661186746840134\n",
      "Norm of gradient:  0.00014799107353000556\n",
      "Cost function:  2.46611844415447\n",
      "Norm of gradient:  0.0001422262922614844\n",
      "Cost function:  2.4661182291039117\n",
      "Norm of gradient:  0.00013667834777223055\n",
      "Cost function:  2.466118016539413\n",
      "Norm of gradient:  0.00013135399794179356\n",
      "Cost function:  2.46611781781792\n",
      "Norm of gradient:  0.000126230450510423\n",
      "Cost function:  2.4661176217953917\n",
      "Norm of gradient:  0.00012131289664567982\n",
      "Cost function:  2.4661174381701265\n",
      "Norm of gradient:  0.00011658125873469196\n",
      "Cost function:  2.4661172573824994\n",
      "Norm of gradient:  0.00011203943213989398\n",
      "Cost function:  2.466117087712708\n",
      "Norm of gradient:  0.00010766970640904759\n",
      "Cost function:  2.466116920958995\n",
      "Norm of gradient:  0.00010347490956887747\n",
      "Cost function:  2.4661167641892745\n",
      "Norm of gradient:  9.94393975308726e-05\n",
      "Cost function:  2.466116610365578\n",
      "Norm of gradient:  9.556512337388515e-05\n",
      "Cost function:  2.4661164655196037\n",
      "Norm of gradient:  9.183824881320266e-05\n",
      "Cost function:  2.466116323610925\n",
      "Norm of gradient:  8.826001372537647e-05\n",
      "Cost function:  2.466116189785639\n",
      "Norm of gradient:  8.481815974279272e-05\n",
      "Cost function:  2.466116058858425\n",
      "Norm of gradient:  8.151334927364597e-05\n",
      "Cost function:  2.466115935218617\n",
      "Norm of gradient:  7.833470789389027e-05\n",
      "Cost function:  2.466115814413982\n",
      "Norm of gradient:  7.528243420925914e-05\n",
      "Cost function:  2.4661157001872724\n",
      "Norm of gradient:  7.234686757131195e-05\n",
      "Cost function:  2.4661155887148074\n",
      "Norm of gradient:  6.952783775576286e-05\n",
      "Cost function:  2.4661154831869854\n",
      "Norm of gradient:  6.681674999491659e-05\n",
      "Cost function:  2.4661153803191143\n",
      "Norm of gradient:  6.421314437983905e-05\n",
      "Cost function:  2.4661152828298185\n",
      "Norm of gradient:  6.170936336508006e-05\n",
      "Cost function:  2.466115187896642\n",
      "Norm of gradient:  5.930472312048621e-05\n",
      "Cost function:  2.466115097835345\n",
      "Norm of gradient:  5.699239129859118e-05\n",
      "Cost function:  2.4661150102199434\n",
      "Norm of gradient:  5.47715145912608e-05\n",
      "Cost function:  2.466114927022228\n",
      "Norm of gradient:  5.2635988208580883e-05\n",
      "Cost function:  2.466114846156345\n",
      "Norm of gradient:  5.058483427528802e-05\n",
      "Cost function:  2.466114769300467\n",
      "Norm of gradient:  4.861259035470342e-05\n",
      "Cost function:  2.4661146946605745\n",
      "Norm of gradient:  4.671819087937614e-05\n",
      "Cost function:  2.4661146236642573\n",
      "Norm of gradient:  4.4896741341511994e-05\n",
      "Cost function:  2.4661145547679526\n",
      "Norm of gradient:  4.3147118595799575e-05\n",
      "Cost function:  2.46611448918544\n",
      "Norm of gradient:  4.146493096956863e-05\n",
      "Cost function:  2.4661144255881284\n",
      "Norm of gradient:  3.984902218797164e-05\n",
      "Cost function:  2.466114365007437\n",
      "Norm of gradient:  3.8295446420916033e-05\n",
      "Cost function:  2.4661143062993256\n",
      "Norm of gradient:  3.680303394380143e-05\n",
      "Cost function:  2.4661142503396953\n",
      "Norm of gradient:  3.5368234813302514e-05\n",
      "Cost function:  2.4661141961430006\n",
      "Norm of gradient:  3.3989881564610674e-05\n",
      "Cost function:  2.4661141444525647\n",
      "Norm of gradient:  3.2664776284349645e-05\n",
      "Cost function:  2.466114094418966\n",
      "Norm of gradient:  3.13917661620478e-05\n",
      "Cost function:  2.466114046672555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of gradient:  3.0167966775379523e-05\n",
      "Cost function:  2.4661140004808715\n",
      "Norm of gradient:  2.899224959741075e-05\n",
      "Cost function:  2.4661139563780097\n",
      "Norm of gradient:  2.786200977626522e-05\n",
      "Cost function:  2.4661139137320305\n",
      "Norm of gradient:  2.677615042264638e-05\n",
      "Cost function:  2.4661138729950745\n",
      "Norm of gradient:  2.5732316360981195e-05\n",
      "Cost function:  2.4661138336216037\n",
      "Norm of gradient:  2.472944779621423e-05\n",
      "Cost function:  2.466113795994003\n",
      "Norm of gradient:  2.3765412856189652e-05\n",
      "Cost function:  2.466113759641048\n",
      "Norm of gradient:  2.2839192742240387e-05\n",
      "Cost function:  2.4661137248857603\n",
      "Norm of gradient:  2.194885557656327e-05\n",
      "Cost function:  2.466113691320852\n",
      "Norm of gradient:  2.1093426195337742e-05\n",
      "Cost function:  2.466113659218869\n",
      "Norm of gradient:  2.027115208260192e-05\n",
      "Cost function:  2.466113628227539\n",
      "Norm of gradient:  1.9481103315631094e-05\n",
      "Cost function:  2.4661135985765195\n",
      "Norm of gradient:  1.8721688456395563e-05\n",
      "Cost function:  2.466113569960891\n",
      "Norm of gradient:  1.799202358684184e-05\n",
      "Cost function:  2.4661135425738805\n",
      "Norm of gradient:  1.7290662132548282e-05\n",
      "Cost function:  2.466113516151375\n",
      "Norm of gradient:  1.6616766275829168e-05\n",
      "Cost function:  2.4661134908556646\n",
      "Norm of gradient:  1.5969019878787634e-05\n",
      "Cost function:  2.4661134664577933\n",
      "Norm of gradient:  1.5346630802192156e-05\n",
      "Cost function:  2.466113443093827\n",
      "Norm of gradient:  1.4748400508833743e-05\n",
      "Cost function:  2.4661134205651134\n",
      "Norm of gradient:  1.4173581690314641e-05\n",
      "Cost function:  2.466113398985481\n",
      "Norm of gradient:  1.3621081974290883e-05\n",
      "Cost function:  2.466113378182433\n",
      "Norm of gradient:  1.3090197721739257e-05\n",
      "Cost function:  2.466113358250965\n",
      "Norm of gradient:  1.257993250857337e-05\n",
      "Cost function:  2.466113339041147\n",
      "Norm of gradient:  1.2089624968843125e-05\n",
      "Cost function:  2.466113320632047\n",
      "Norm of gradient:  1.1618365500404915e-05\n",
      "Cost function:  2.466113302893227\n",
      "Norm of gradient:  1.116553343724676e-05\n",
      "Cost function:  2.46611328589029\n",
      "Norm of gradient:  1.0730297821028893e-05\n",
      "Cost function:  2.4661132695096413\n",
      "Norm of gradient:  1.0312077001124319e-05\n",
      "Cost function:  2.4661132538055224\n",
      "Norm of gradient:  9.9101113363127e-06\n",
      "Cost function:  2.4661132386788993\n",
      "Norm of gradient:  9.523856422075574e-06\n",
      "Cost function:  2.46611322417444\n",
      "Norm of gradient:  9.152617358442647e-06\n",
      "Cost function:  2.466113210205706\n",
      "Norm of gradient:  8.795885180997147e-06\n",
      "Cost function:  2.4661131968093057\n",
      "Norm of gradient:  8.453023820824138e-06\n",
      "Cost function:  2.4661131839097177\n",
      "Norm of gradient:  8.123557932051296e-06\n",
      "Cost function:  2.466113171536767\n",
      "Norm of gradient:  7.806904956433938e-06\n",
      "Cost function:  2.466113159624392\n",
      "Norm of gradient:  7.502621361395666e-06\n",
      "Cost function:  2.4661131481967375\n",
      "Norm of gradient:  7.210173301017306e-06\n",
      "Cost function:  2.466113137195944\n",
      "Norm of gradient:  6.929147275975639e-06\n",
      "Cost function:  2.4661131266413903\n",
      "Norm of gradient:  6.6590538269160826e-06\n",
      "Cost function:  2.4661131164823478\n",
      "Norm of gradient:  6.3995077534195015e-06\n",
      "Cost function:  2.466113106734217\n",
      "Norm of gradient:  6.150060068390575e-06\n",
      "Cost function:  2.466113097352453\n",
      "Norm of gradient:  5.9103521843207396e-06\n",
      "Cost function:  2.466113088349152\n",
      "Norm of gradient:  5.67997205595361e-06\n",
      "Cost function:  2.466113079685144\n",
      "Norm of gradient:  5.458586078996844e-06\n",
      "Cost function:  2.466113071369781\n",
      "Norm of gradient:  5.245815948985274e-06\n",
      "Cost function:  2.4661130633685726\n",
      "Norm of gradient:  5.0413514846840955e-06\n",
      "Cost function:  2.466113055688596\n",
      "Norm of gradient:  4.844845222011326e-06\n",
      "Cost function:  2.466113048299449\n",
      "Norm of gradient:  4.656008908913065e-06\n",
      "Cost function:  2.4661130412063192\n",
      "Norm of gradient:  4.47452328537385e-06\n",
      "Cost function:  2.466113034382376\n",
      "Norm of gradient:  4.30012061511066e-06\n",
      "Cost function:  2.466113027831269\n",
      "Norm of gradient:  4.132507443910438e-06\n",
      "Cost function:  2.466113021529268\n",
      "Norm of gradient:  3.971435202575269e-06\n",
      "Cost function:  2.466113015478771\n",
      "Norm of gradient:  3.816634070739963e-06\n",
      "Cost function:  2.46611300965877\n",
      "Norm of gradient:  3.6678733647925734e-06\n",
      "Cost function:  2.4661130040706367\n",
      "Norm of gradient:  3.5249049192978626e-06\n",
      "Cost function:  2.4661129986957464\n",
      "Norm of gradient:  3.3875147322730384e-06\n",
      "Cost function:  2.4661129935346535\n",
      "Norm of gradient:  3.255474487082047e-06\n",
      "Cost function:  2.4661129885708166\n",
      "Norm of gradient:  3.1285857242937204e-06\n",
      "Cost function:  2.466112983804134\n",
      "Norm of gradient:  3.006638334851928e-06\n",
      "Cost function:  2.4661129792199024\n",
      "Norm of gradient:  2.8894483298967293e-06\n",
      "Cost function:  2.466112974817496\n",
      "Norm of gradient:  2.7768223053711687e-06\n",
      "Cost function:  2.4661129705838225\n",
      "Norm of gradient:  2.6685897401640747e-06\n",
      "Cost function:  2.4661129665178567\n",
      "Norm of gradient:  2.5645725658071875e-06\n",
      "Cost function:  2.4661129626079257\n",
      "Norm of gradient:  2.4646127848047303e-06\n",
      "Cost function:  2.4661129588526967\n",
      "Norm of gradient:  2.368546410440799e-06\n",
      "Cost function:  2.4661129552417407\n",
      "Norm of gradient:  2.27622708678581e-06\n",
      "Cost function:  2.4661129517735016\n",
      "Norm of gradient:  2.1875037653263887e-06\n",
      "Cost function:  2.466112948438652\n",
      "Norm of gradient:  2.1022409029356105e-06\n",
      "Cost function:  2.4661129452354733\n",
      "Norm of gradient:  2.020299344783493e-06\n",
      "Cost function:  2.466112942155614\n",
      "Norm of gradient:  1.941553585069561e-06\n",
      "Cost function:  2.4661129391972394\n",
      "Norm of gradient:  1.865875402681582e-06\n",
      "Cost function:  2.4661129363528644\n",
      "Norm of gradient:  1.7931486129883409e-06\n",
      "Cost function:  2.4661129336205905\n",
      "Norm of gradient:  1.7232550459422667e-06\n",
      "Cost function:  2.4661129309936864\n",
      "Norm of gradient:  1.6560871662806128e-06\n",
      "Cost function:  2.466112928470234\n",
      "Norm of gradient:  1.5915360527965983e-06\n",
      "Cost function:  2.4661129260441763\n",
      "Norm of gradient:  1.529502186221954e-06\n",
      "Cost function:  2.4661129237135837\n",
      "Norm of gradient:  1.469885160574898e-06\n",
      "Cost function:  2.4661129214730098\n",
      "Norm of gradient:  1.4125928902200086e-06\n",
      "Cost function:  2.466112919320542\n",
      "Norm of gradient:  1.3575328016557517e-06\n",
      "Cost function:  2.4661129172512672\n",
      "Norm of gradient:  1.3046197021008082e-06\n",
      "Cost function:  2.4661129152633112\n",
      "Norm of gradient:  1.253768230130173e-06\n",
      "Cost function:  2.4661129133522364\n",
      "Norm of gradient:  1.2048995777964023e-06\n",
      "Cost function:  2.4661129115162166\n",
      "Norm of gradient:  1.157935026878637e-06\n",
      "Cost function:  2.466112909751246\n",
      "Norm of gradient:  1.1128016844881234e-06\n",
      "Cost function:  2.466112908055555\n",
      "Norm of gradient:  1.0694269486353626e-06\n",
      "Cost function:  2.4661129064255163\n",
      "Norm of gradient:  1.0277434051814947e-06\n",
      "Cost function:  2.4661129048594264\n",
      "Norm of gradient:  9.876840884303935e-07\n",
      "Cost function:  2.466112903354003\n",
      "Norm of gradient:  9.491866589015667e-07\n",
      "Cost function:  2.466112901907611\n",
      "Norm of gradient:  9.121893400426856e-07\n",
      "Cost function:  2.466112900517272\n",
      "Norm of gradient:  8.766344908983561e-07\n",
      "Cost function:  2.46611289918143\n",
      "Norm of gradient:  8.424651203384169e-07\n",
      "Cost function:  2.4661128978973803\n",
      "Norm of gradient:  8.096279351642828e-07\n",
      "Cost function:  2.4661128966636383\n",
      "Norm of gradient:  7.780703527081501e-07\n",
      "Cost function:  2.4661128954777496\n",
      "Norm of gradient:  7.477431039910894e-07\n",
      "Cost function:  2.466112894338303\n",
      "Norm of gradient:  7.185976725975788e-07\n",
      "Cost function:  2.46611289324307\n",
      "Norm of gradient:  6.905885127337623e-07\n",
      "Cost function:  2.466112892190715\n"
     ]
    }
   ],
   "source": [
    "# Reduced gradient iteration\n",
    "\n",
    "max_iter = 300\n",
    "xk = np.copy(x)\n",
    "uk = np.copy(u)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    \n",
    "    # power flow\n",
    "    xk = powerflow(xk, uk, p)\n",
    "\n",
    "    # lambda calculation\n",
    "    J_x = gfun_x(xk, uk, p)\n",
    "    G_x = cfun_x(xk, uk, p)\n",
    "\n",
    "    lam = -np.dot(inv(np.transpose(J_x)), G_x)\n",
    "\n",
    "    # gradient cost function\n",
    "    J_u = gfun_u(xk, uk, p)\n",
    "    G_u = cfun_u(xk, uk, p)\n",
    "\n",
    "    grad_c = G_u + np.dot(np.transpose(J_u), lam)\n",
    "    print(\"Norm of gradient: \", np.linalg.norm(grad_c))\n",
    "    \n",
    "    # evaluate cost function\n",
    "    print(\"Cost function: \", cfun(xk, uk, p))    \n",
    "    \n",
    "    # compute step\n",
    "    alpha = 0.08\n",
    "    uk = uk - alpha*grad_c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
