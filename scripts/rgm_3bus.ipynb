{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import sin, cos\n",
    "import numba as nb\n",
    "from scipy.optimize import fsolve\n",
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# power flow function (equality constraints)\n",
    "def gfun(x, u, p):\n",
    "  \n",
    "    VM3 = x[0]\n",
    "    VA3 = x[1]\n",
    "    VA2 = x[2]\n",
    "    \n",
    "    VM1 = u[0]\n",
    "    P2 = u[1]\n",
    "    VM2 = u[2]\n",
    "    \n",
    "    VA1 = p[0]\n",
    "    P3 = p[1]\n",
    "    Q3 = p[2]\n",
    "\n",
    "    # intermediate quantities\n",
    "    VA23 = VA2 - VA3\n",
    "    VA31 = VA3 - VA1\n",
    "    VA32 = VA3 - VA2\n",
    "    \n",
    "    F1 = 4.0*VM2*VM2 + VM2*VM3*(-4*cos(VA23) + 10*sin(VA23)) - P2\n",
    "    F2 = (8.0*VM3*VM3 + VM3*VM1*(-4*cos(VA31) + 5*sin(VA31))\n",
    "          + VM3*VM2*(-4*cos(VA32) + 10*sin(VA32)) + P3)\n",
    "    F3 = (15.0*VM3*VM3 + VM3*VM1*(-4*sin(VA31) - 5*cos(VA31))\n",
    "          + VM3*VM2*(-4*sin(VA32) - 10*cos(VA32)) + Q3)\n",
    "\n",
    "    return np.array([F1, F2, F3])\n",
    "\n",
    "# cost function\n",
    "def cfun(x, u, p):\n",
    "\n",
    "    VM3 = x[0]\n",
    "    VA3 = x[1]\n",
    "\n",
    "    VM1 = u[0]\n",
    "    P2 = u[1]\n",
    "\n",
    "    VA1 = p[0]\n",
    "\n",
    "    VA13 = VA1 - VA3\n",
    "    \n",
    "    w1 = 1.0\n",
    "    w2 = 1.0\n",
    "\n",
    "    cost = (w1*(4.0*VM1*VM1 + VM1*VM3*(-4*cos(VA13) + 5*sin(VA13))) +\n",
    "          w2*P2)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobians and gradients\n",
    "\n",
    "def gfun_x(x, u, p):\n",
    "\n",
    "    \n",
    "    VM3 = x[0]\n",
    "    VA3 = x[1]\n",
    "    VA2 = x[2]\n",
    "    \n",
    "    VM1 = u[0]\n",
    "    P2 = u[1]\n",
    "    VM2 = u[2]\n",
    "    \n",
    "    VA1 = p[0]\n",
    "    P3 = p[1]\n",
    "    Q3 = p[2]\n",
    "\n",
    "    # intermediate quantities\n",
    "    VA23 = VA2 - VA3\n",
    "    VA31 = VA3 - VA1\n",
    "    VA32 = VA3 - VA2\n",
    "    \n",
    "    J = np.zeros((3, 3))\n",
    "    \n",
    "    #F1\n",
    "    J[0, 0] =  VM2*(10*sin(VA2 - VA3) - 4*cos(VA2 - VA3))\n",
    "    J[0, 1] =  VM2*VM3*(-4*sin(VA2 - VA3) - 10*cos(VA2 - VA3))\n",
    "    J[0, 2] =  VM2*VM3*(4*sin(VA2 - VA3) + 10*cos(VA2 - VA3))\n",
    "    #F2\n",
    "    J[1, 0] =  VM1*(-5*sin(VA1 - VA3) - 4*cos(VA1 - VA3)) + VM2*(-10*sin(VA2 - VA3) - 4*cos(VA2 - VA3)) + 16.0*VM3\n",
    "    J[1, 1] =  VM1*VM3*(-4*sin(VA1 - VA3) + 5*cos(VA1 - VA3)) + VM2*VM3*(-4*sin(VA2 - VA3) + 10*cos(VA2 - VA3))\n",
    "    J[1, 2] =  VM2*VM3*(4*sin(VA2 - VA3) - 10*cos(VA2 - VA3))\n",
    "    #F3\n",
    "    J[2, 0] =  VM1*(4*sin(VA1 - VA3) - 5*cos(VA1 - VA3)) + VM2*(4*sin(VA2 - VA3) - 10*cos(VA2 - VA3)) + 30.0*VM3\n",
    "    J[2, 1] =  VM1*VM3*(-5*sin(VA1 - VA3) - 4*cos(VA1 - VA3)) + VM2*VM3*(-10*sin(VA2 - VA3) - 4*cos(VA2 - VA3))\n",
    "    J[2, 2] =  VM2*VM3*(10*sin(VA2 - VA3) + 4*cos(VA2 - VA3))\n",
    "\n",
    "    return J\n",
    "\n",
    "def gfun_u(x, u, p):\n",
    "    \n",
    "    VM3 = x[0]\n",
    "    VA3 = x[1]\n",
    "    VA2 = x[2]\n",
    "    \n",
    "    VM1 = u[0]\n",
    "    P2 = u[1]\n",
    "    VM2 = u[2]\n",
    "    \n",
    "    VA1 = p[0]\n",
    "    P3 = p[1]\n",
    "    Q3 = p[2]\n",
    "\n",
    "    # intermediate quantities\n",
    "    VA23 = VA2 - VA3\n",
    "    VA31 = VA3 - VA1\n",
    "    VA32 = VA3 - VA2\n",
    "    \n",
    "    J = np.zeros((3, 3))\n",
    "    \n",
    "    #F1\n",
    "    J[0, 0] =  0\n",
    "    J[0, 1] =  -1\n",
    "    J[0, 2] =  8.0*VM2 + VM3*(10*sin(VA2 - VA3) - 4*cos(VA2 - VA3))\n",
    "    #F2\n",
    "    J[1, 0] =  VM3*(-5*sin(VA1 - VA3) - 4*cos(VA1 - VA3))\n",
    "    J[1, 1] =  0\n",
    "    J[1, 2] =  VM3*(-10*sin(VA2 - VA3) - 4*cos(VA2 - VA3))\n",
    "    #F3\n",
    "    J[2, 0] =  VM3*(4*sin(VA1 - VA3) - 5*cos(VA1 - VA3))\n",
    "    J[2, 1] =  0\n",
    "    J[2, 2] =  VM3*(4*sin(VA2 - VA3) - 10*cos(VA2 - VA3))\n",
    "\n",
    "    return J\n",
    "\n",
    "\n",
    "def cfun_x(x, u, p):\n",
    "    \n",
    "    VM3 = x[0]\n",
    "    VA3 = x[1]\n",
    "\n",
    "    VM1 = u[0]\n",
    "    P2 = u[1]\n",
    "\n",
    "    VA1 = p[0]\n",
    "\n",
    "    VA13 = VA1 - VA3\n",
    "    \n",
    "    w1 = 1.0\n",
    "    w2 = 1.0\n",
    "    \n",
    "    grad = np.zeros(3)\n",
    "    grad[0] =  VM1*w1*(5*sin(VA1 - VA3) - 4*cos(VA1 - VA3))\n",
    "    grad[1] =  VM1*VM3*w1*(-4*sin(VA1 - VA3) - 5*cos(VA1 - VA3))\n",
    "    grad[2] =  0\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def cfun_u(x, u, p):\n",
    "    \n",
    "    VM3 = x[0]\n",
    "    VA3 = x[1]\n",
    "\n",
    "    VM1 = u[0]\n",
    "    P2 = u[1]\n",
    "\n",
    "    VA1 = p[0]\n",
    "\n",
    "    VA13 = VA1 - VA3\n",
    "    \n",
    "    w1 = 1.0\n",
    "    w2 = 1.0\n",
    "    \n",
    "    grad = np.zeros(3)\n",
    "    grad[0] =  w1*(8.0*VM1 + VM3*(5*sin(VA1 - VA3) - 4*cos(VA1 - VA3)))\n",
    "    grad[1] =  w2\n",
    "    grad[2] =  0\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize script with same initial conditions as in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n",
      "[1.  1.7 1. ]\n"
     ]
    }
   ],
   "source": [
    "# initial parameters\n",
    "x = np.zeros(3)\n",
    "u = np.zeros(3)\n",
    "p = np.zeros(3)\n",
    "\n",
    "# this is an initial guess\n",
    "x[0] = 1.0 #VM3\n",
    "x[1] = 0.0 #VA3\n",
    "x[2] = 0.0 #VA2\n",
    "\n",
    "# this is given by the problem data, but might be \"controlled\" via OPF\n",
    "u[0] = 1.0 #VM1\n",
    "u[1] = 1.7 #P2\n",
    "u[2] = 1.0 #VM2\n",
    "\n",
    "# these parameters are fixed through the computation\n",
    "p[0] = 0.0 #VA1, slack angle\n",
    "p[1] = 2.0 #P3\n",
    "p[2] = 1.0 #Q3\n",
    "\n",
    "# print initial guesses\n",
    "print(x)\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.88186783 -0.00094814  0.1349708 ]\n"
     ]
    }
   ],
   "source": [
    "# POWER FLOW ALGO\n",
    "\n",
    "def powerflow(x, u, p):\n",
    "    \n",
    "    sol = fsolve(gfun, x, args=(u,p,))\n",
    "    return sol\n",
    "    \n",
    "print(powerflow(x, u, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of gradient:  0.7038831243894311\n",
      "Cost function:  2.176710951198614\n",
      "Norm of gradient:  0.8340827610266326\n",
      "Cost function:  2.1695645329988715\n",
      "Norm of gradient:  0.951099610826598\n",
      "Cost function:  2.1719621683744093\n",
      "Norm of gradient:  1.108030972661971\n",
      "Cost function:  2.1693946680543004\n",
      "Norm of gradient:  1.230367006528185\n",
      "Cost function:  2.1830303407946587\n",
      "Norm of gradient:  1.395553609142824\n",
      "Cost function:  2.1795811561848035\n",
      "Norm of gradient:  1.5127874416652054\n",
      "Cost function:  2.205675180920455\n",
      "Norm of gradient:  1.6750702299646125\n",
      "Cost function:  2.1963007922332323\n",
      "Norm of gradient:  1.7813388013505258\n",
      "Cost function:  2.236399099495717\n",
      "Norm of gradient:  1.931440441373678\n",
      "Cost function:  2.2163736979091953\n",
      "Norm of gradient:  2.022851899089019\n",
      "Cost function:  2.2711850664487847\n",
      "Norm of gradient:  2.1527366259678313\n",
      "Cost function:  2.23684780970913\n",
      "Norm of gradient:  2.2267010862448573\n",
      "Cost function:  2.3055342833291252\n",
      "Norm of gradient:  2.329950251703329\n",
      "Cost function:  2.2550857182267414\n",
      "Norm of gradient:  2.384962652395282\n",
      "Cost function:  2.335011376863811\n",
      "Norm of gradient:  2.4573248686544256\n",
      "Cost function:  2.268970118664371\n",
      "Norm of gradient:  2.492871601415813\n",
      "Cost function:  2.3559633198927425\n",
      "Norm of gradient:  2.5326324043352946\n",
      "Cost function:  2.2770871146284435\n",
      "Norm of gradient:  2.549110520210494\n",
      "Cost function:  2.366127966175351\n",
      "Norm of gradient:  2.557100155166649\n",
      "Cost function:  2.278815051267078\n",
      "Norm of gradient:  2.5557023853311547\n",
      "Cost function:  2.364913882324304\n",
      "Norm of gradient:  2.5349167145595644\n",
      "Cost function:  2.2742904347366606\n",
      "Norm of gradient:  2.5174706026632707\n",
      "Cost function:  2.3532765880222355\n",
      "Norm of gradient:  2.4724167094892393\n",
      "Cost function:  2.264263582442678\n",
      "Norm of gradient:  2.441190054198869\n",
      "Cost function:  2.333278700121422\n",
      "Norm of gradient:  2.3771374466330233\n",
      "Cost function:  2.249887131262125\n",
      "Norm of gradient:  2.3346346482918103\n",
      "Cost function:  2.3075184720302975\n",
      "Norm of gradient:  2.2569404437601457\n",
      "Cost function:  2.2324911667657745\n",
      "Norm of gradient:  2.205715234084873\n",
      "Cost function:  2.2786082036189885\n",
      "Norm of gradient:  2.1193258453268045\n",
      "Cost function:  2.213389789132984\n",
      "Norm of gradient:  2.0618287922127156\n",
      "Cost function:  2.2488115750028093\n",
      "Norm of gradient:  1.9709854220859695\n",
      "Cost function:  2.1937441655259375\n",
      "Norm of gradient:  1.9094546210802734\n",
      "Cost function:  2.2198639071647426\n",
      "Norm of gradient:  1.8175760116220745\n",
      "Cost function:  2.1744869582543336\n",
      "Norm of gradient:  1.7539705553935923\n",
      "Cost function:  2.192941460612175\n",
      "Norm of gradient:  1.6636622977717077\n",
      "Cost function:  2.1562990407206026\n",
      "Norm of gradient:  1.5996323824499448\n",
      "Cost function:  2.1687243434578276\n",
      "Norm of gradient:  1.5127709201865875\n",
      "Cost function:  2.13962315198544\n",
      "Norm of gradient:  1.4496559188902416\n",
      "Cost function:  2.1475018896601896\n",
      "Norm of gradient:  1.3675060562454837\n",
      "Cost function:  2.124698866516324\n",
      "Norm of gradient:  1.3063517413809775\n",
      "Cost function:  2.12928471696694\n",
      "Norm of gradient:  1.2296901625196839\n",
      "Cost function:  2.1116062397963646\n",
      "Norm of gradient:  1.1712774590464685\n",
      "Cost function:  2.1139036588717\n",
      "Norm of gradient:  1.1005066897265368\n",
      "Cost function:  2.1003094969953855\n",
      "Norm of gradient:  1.0453860691437422\n",
      "Cost function:  2.1010877454204273\n",
      "Norm of gradient:  0.9806318585603861\n",
      "Cost function:  2.0906957902894105\n",
      "Norm of gradient:  0.9291592061243134\n",
      "Cost function:  2.090520654275164\n",
      "Norm of gradient:  0.8703496145672066\n",
      "Cost function:  2.0826068227133145\n",
      "Norm of gradient:  0.8227208669535088\n",
      "Cost function:  2.08187857352257\n",
      "Norm of gradient:  0.7696482052800917\n",
      "Cost function:  2.075862953231489\n",
      "Norm of gradient:  0.7259311774895432\n",
      "Cost function:  2.0748535696970727\n",
      "Norm of gradient:  0.678299199209418\n",
      "Cost function:  2.070280428472488\n",
      "Norm of gradient:  0.6384618161999738\n",
      "Cost function:  2.069166444681277\n",
      "Norm of gradient:  0.5959208814946105\n",
      "Cost function:  2.0656828579269746\n",
      "Norm of gradient:  0.559855557781359\n",
      "Cost function:  2.0645724199191995\n",
      "Norm of gradient:  0.5220283295849925\n",
      "Cost function:  2.061908171029088\n",
      "Norm of gradient:  0.48957256943855676\n",
      "Cost function:  2.0608622118427338\n",
      "Norm of gradient:  0.45607243736679115\n",
      "Cost function:  2.0588122160043607\n",
      "Norm of gradient:  0.4270259129417425\n",
      "Cost function:  2.057860351981069\n",
      "Norm of gradient:  0.3974699288373974\n",
      "Cost function:  2.0562699931454906\n",
      "Norm of gradient:  0.37160838840456545\n",
      "Cost function:  2.0554220261188334\n",
      "Norm of gradient:  0.34562611316340197\n",
      "Cost function:  2.054175318501594\n",
      "Norm of gradient:  0.3227125037716301\n",
      "Cost function:  2.0534292718206366\n",
      "Norm of gradient:  0.2999518355088746\n",
      "Cost function:  2.0524395252748526\n",
      "Norm of gradient:  0.27974502242619614\n",
      "Cost function:  2.051787058549457\n",
      "Norm of gradient:  0.2598758111475882\n",
      "Cost function:  2.0509896459063977\n",
      "Norm of gradient:  0.24213725303821163\n",
      "Cost function:  2.050419558008746\n",
      "Norm of gradient:  0.22485329903931323\n",
      "Cost function:  2.049766383351617\n",
      "Norm of gradient:  0.20935200331760248\n",
      "Cost function:  2.049266767320093\n",
      "Norm of gradient:  0.19437187667602554\n",
      "Cost function:  2.048722076425972\n",
      "Norm of gradient:  0.18088791911675964\n",
      "Cost function:  2.0482815544061275\n",
      "Norm of gradient:  0.1679549158222784\n",
      "Cost function:  2.0478187823728193\n",
      "Norm of gradient:  0.1562817670167871\n",
      "Cost function:  2.047427136385773\n",
      "Norm of gradient:  0.1451632243207711\n",
      "Cost function:  2.047026549617893\n",
      "Norm of gradient:  0.13510908093226687\n",
      "Cost function:  2.0466749714831503\n",
      "Norm of gradient:  0.1255952022718668\n",
      "Cost function:  2.0463219078621897\n",
      "Norm of gradient:  0.11698348482936284\n",
      "Cost function:  2.0460030215649834\n",
      "Norm of gradient:  0.10888577518054261\n",
      "Cost function:  2.045686581603144\n",
      "Norm of gradient:  0.10155491925884438\n",
      "Cost function:  2.045394341525951\n",
      "Norm of gradient:  0.0947042975295932\n",
      "Cost function:  2.0451064156004604\n",
      "Norm of gradient:  0.0885069491420232\n",
      "Cost function:  2.0448359461346683\n",
      "Norm of gradient:  0.08275159911432126\n",
      "Cost function:  2.044570492313108\n",
      "Norm of gradient:  0.07755333285769958\n",
      "Cost function:  2.044317909128058\n",
      "Norm of gradient:  0.07275638156649011\n",
      "Cost function:  2.044070417335339\n",
      "Norm of gradient:  0.06843410249174396\n",
      "Cost function:  2.043832653411429\n",
      "Norm of gradient:  0.06447127956912928\n",
      "Cost function:  2.04359974802908\n",
      "Norm of gradient:  0.060911543603115945\n",
      "Cost function:  2.043374396395059\n",
      "Norm of gradient:  0.05766906051103806\n",
      "Cost function:  2.0431535416579134\n",
      "Norm of gradient:  0.05476662295154624\n",
      "Cost function:  2.042938719798636\n",
      "Norm of gradient:  0.05213956868033071\n",
      "Cost function:  2.042728001515774\n",
      "Norm of gradient:  0.04979648290884179\n",
      "Cost function:  2.0425222382869377\n",
      "Norm of gradient:  0.047687991707075446\n",
      "Cost function:  2.042320202225974\n",
      "Norm of gradient:  0.04581346510129997\n",
      "Cost function:  2.0421223458518\n",
      "Norm of gradient:  0.04413472905004373\n",
      "Cost function:  2.0419278781820576\n",
      "Norm of gradient:  0.04264569647132159\n",
      "Cost function:  2.0417370228373573\n",
      "Norm of gradient:  0.041316620278985\n",
      "Cost function:  2.041549261778528\n",
      "Norm of gradient:  0.04013871957001505\n",
      "Cost function:  2.041364689900673\n",
      "Norm of gradient:  0.03908878947760178\n",
      "Cost function:  2.0411829605081113\n",
      "Norm of gradient:  0.03815727472717302\n",
      "Cost function:  2.0410040980379747\n",
      "Norm of gradient:  0.037326173883325674\n",
      "Cost function:  2.0408278641249695\n",
      "Norm of gradient:  0.03658636625227133\n",
      "Cost function:  2.0406542461391695\n",
      "Norm of gradient:  0.035924029247459244\n",
      "Cost function:  2.040483074875711\n",
      "Norm of gradient:  0.035331124853717234\n",
      "Cost function:  2.0403143194240174\n",
      "Norm of gradient:  0.03479717141846803\n",
      "Cost function:  2.0401478552977315\n",
      "Norm of gradient:  0.034315461397318586\n",
      "Cost function:  2.0399836436264316\n",
      "Norm of gradient:  0.03387814775434382\n",
      "Cost function:  2.0398215893056237\n",
      "Norm of gradient:  0.033479849307181855\n",
      "Cost function:  2.0396616509917544\n",
      "Norm of gradient:  0.03311476032417367\n",
      "Cost function:  2.0395037532668616\n",
      "Norm of gradient:  0.03277868750267037\n",
      "Cost function:  2.039347855091038\n",
      "Norm of gradient:  0.03246737872667138\n",
      "Cost function:  2.0391938945452375\n",
      "Norm of gradient:  0.0321776364663923\n",
      "Cost function:  2.039041832186304\n",
      "Norm of gradient:  0.03190637211457945\n",
      "Cost function:  2.038891615598825\n",
      "Norm of gradient:  0.03165118650717821\n",
      "Cost function:  2.038743207442918\n",
      "Norm of gradient:  0.03140985052306208\n",
      "Cost function:  2.038596562191104\n",
      "Norm of gradient:  0.03118058525682708\n",
      "Cost function:  2.0384516447155505\n",
      "Norm of gradient:  0.030961789380713978\n",
      "Cost function:  2.0383084146363784\n",
      "Norm of gradient:  0.030752154958289816\n",
      "Cost function:  2.0381668389603265\n",
      "Norm of gradient:  0.030550535071948385\n",
      "Cost function:  2.038026881275996\n",
      "Norm of gradient:  0.03035597336131532\n",
      "Cost function:  2.037888510572328\n",
      "Norm of gradient:  0.03016764943504426\n",
      "Cost function:  2.037751693591156\n",
      "Norm of gradient:  0.029984866038843278\n",
      "Cost function:  2.0376164011320004\n",
      "Norm of gradient:  0.029807036052449033\n",
      "Cost function:  2.0374826025138844\n",
      "Norm of gradient:  0.029633651329504792\n",
      "Cost function:  2.037350270181752\n",
      "Norm of gradient:  0.029464290627832235\n",
      "Cost function:  2.0372193756150447\n",
      "Norm of gradient:  0.029298582807447585\n",
      "Cost function:  2.0370898927556276\n",
      "Norm of gradient:  0.029136224151338354\n",
      "Cost function:  2.036961794934462\n",
      "Norm of gradient:  0.028976942236480775\n",
      "Cost function:  2.03683505746066\n",
      "Norm of gradient:  0.028820516382458156\n",
      "Cost function:  2.036709655282049\n",
      "Norm of gradient:  0.028666745020075775\n",
      "Cost function:  2.0365855649631355\n",
      "Norm of gradient:  0.02851546602621957\n",
      "Cost function:  2.0364627628858427\n",
      "Norm of gradient:  0.028366528565693674\n",
      "Cost function:  2.0363412267735876\n",
      "Norm of gradient:  0.028219811770259805\n",
      "Cost function:  2.0362209342966646\n",
      "Norm of gradient:  0.02807520109904789\n",
      "Cost function:  2.0361018642536863\n",
      "Norm of gradient:  0.02793260475314211\n",
      "Cost function:  2.0359839954843557\n",
      "Norm of gradient:  0.027791934173225317\n",
      "Cost function:  2.0358673077891853\n",
      "Norm of gradient:  0.027653118074930944\n",
      "Cost function:  2.0357517810782166\n",
      "Norm of gradient:  0.02751608654311809\n",
      "Cost function:  2.0356373960889034\n",
      "Norm of gradient:  0.02738078281858684\n",
      "Cost function:  2.035524133717484\n",
      "Norm of gradient:  0.02724715042490044\n",
      "Cost function:  2.0354119755802325\n",
      "Norm of gradient:  0.02711514294446257\n",
      "Cost function:  2.035300903486974\n",
      "Norm of gradient:  0.026984713653676714\n",
      "Cost function:  2.0351908998801584\n",
      "Norm of gradient:  0.0268558235595993\n",
      "Cost function:  2.0350819474196427\n",
      "Norm of gradient:  0.026728433084279624\n",
      "Cost function:  2.0349740293260696\n",
      "Norm of gradient:  0.02660250862805622\n",
      "Cost function:  2.0348671290529174\n",
      "Norm of gradient:  0.026478015914222004\n",
      "Cost function:  2.034761230554928\n",
      "Norm of gradient:  0.02635492532229353\n",
      "Cost function:  2.034656318028718\n",
      "Norm of gradient:  0.026233206570436315\n",
      "Cost function:  2.034552376122369\n",
      "Norm of gradient:  0.026112833032148775\n",
      "Cost function:  2.034449389730015\n",
      "Norm of gradient:  0.025993777492616513\n",
      "Cost function:  2.034347344155247\n",
      "Norm of gradient:  0.025876015631486622\n",
      "Cost function:  2.03424622494821\n",
      "Norm of gradient:  0.02575952263905015\n",
      "Cost function:  2.034146018032721\n",
      "Norm of gradient:  0.025644276018797436\n",
      "Cost function:  2.034046709576981\n",
      "Norm of gradient:  0.02553025289084148\n",
      "Cost function:  2.0339482860921247\n",
      "Norm of gradient:  0.025417432242044714\n",
      "Cost function:  2.033850734329447\n",
      "Norm of gradient:  0.02530579277809136\n",
      "Cost function:  2.033754041356616\n",
      "Norm of gradient:  0.025195314726415598\n",
      "Cost function:  2.033658194475742\n",
      "Norm of gradient:  0.02508597812597325\n",
      "Cost function:  2.0335631812821577\n",
      "Norm of gradient:  0.024977764269256283\n",
      "Cost function:  2.033468989598928\n",
      "Norm of gradient:  0.024870654340923056\n",
      "Cost function:  2.0333756075219345\n",
      "Norm of gradient:  0.02476463056930214\n",
      "Cost function:  2.033283023367456\n",
      "Norm of gradient:  0.024659675142884266\n",
      "Cost function:  2.033191225706468\n",
      "Norm of gradient:  0.02455577112847424\n",
      "Cost function:  2.033100203322583\n",
      "Norm of gradient:  0.024452901609001206\n",
      "Cost function:  2.033009945238006\n",
      "Norm of gradient:  0.024351050414441158\n",
      "Cost function:  2.0329204406795576\n",
      "Norm of gradient:  0.024250201435895995\n",
      "Cost function:  2.0328316790981398\n",
      "Norm of gradient:  0.02415033920659848\n",
      "Cost function:  2.0327436501412586\n",
      "Norm of gradient:  0.024051448358686525\n",
      "Cost function:  2.0326563437176066\n",
      "Norm of gradient:  0.023953514074507826\n",
      "Cost function:  2.032569749772688\n",
      "Norm of gradient:  0.023856521667445285\n",
      "Cost function:  2.0324838586041336\n",
      "Norm of gradient:  0.02376045693743273\n",
      "Cost function:  2.0323986606383038\n",
      "Norm of gradient:  0.023665305828095957\n",
      "Cost function:  2.0323141464904992\n",
      "Norm of gradient:  0.023571054716047507\n",
      "Cost function:  2.032230306948569\n",
      "Norm of gradient:  0.023477690136025586\n",
      "Cost function:  2.0321471329788374\n",
      "Norm of gradient:  0.023385199009473098\n",
      "Cost function:  2.032064615712648\n",
      "Norm of gradient:  0.023293568425787226\n",
      "Cost function:  2.0319827464509927\n",
      "Norm of gradient:  0.023202785822575518\n",
      "Cost function:  2.031901516652807\n",
      "Norm of gradient:  0.0231128388114544\n",
      "Cost function:  2.031820917937532\n",
      "Norm of gradient:  0.02302371531996023\n",
      "Cost function:  2.0317409420759915\n",
      "Norm of gradient:  0.022935403452771598\n",
      "Cost function:  2.0316615809915683\n",
      "Norm of gradient:  0.022847891603140772\n",
      "Cost function:  2.0315828267526106\n",
      "Norm of gradient:  0.02276116834225201\n",
      "Cost function:  2.0315046715722516\n",
      "Norm of gradient:  0.022675222506505676\n",
      "Cost function:  2.0314271078022466\n",
      "Norm of gradient:  0.022590043109279524\n",
      "Cost function:  2.0313501279326625\n",
      "Norm of gradient:  0.02250561940903733\n",
      "Cost function:  2.0312737245857404\n",
      "Norm of gradient:  0.02242194083886371\n",
      "Cost function:  2.0311978905157684\n",
      "Norm of gradient:  0.02233899705948658\n",
      "Cost function:  2.031122618603478\n",
      "Norm of gradient:  0.02225677790281815\n",
      "Cost function:  2.031047901855196\n",
      "Norm of gradient:  0.022175273413171537\n",
      "Cost function:  2.0309737333986684\n",
      "Norm of gradient:  0.022094473801891844\n",
      "Cost function:  2.030900106481316\n",
      "Norm of gradient:  0.02201436947922767\n",
      "Cost function:  2.0308270144667855\n",
      "Norm of gradient:  0.021934951017805578\n",
      "Cost function:  2.0307544508329656\n",
      "Norm of gradient:  0.0218562091771381\n",
      "Cost function:  2.030682409169108\n",
      "Norm of gradient:  0.02177813487413745\n",
      "Cost function:  2.0306108831736847\n",
      "Norm of gradient:  0.021700719201770118\n",
      "Cost function:  2.030539866651529\n",
      "Norm of gradient:  0.021623953405230583\n",
      "Cost function:  2.0304693535121827\n",
      "Norm of gradient:  0.02154782889605498\n",
      "Cost function:  2.030399337767008\n",
      "Norm of gradient:  0.021472337232676764\n",
      "Cost function:  2.030329813527418\n",
      "Norm of gradient:  0.02139747013099262\n",
      "Cost function:  2.0302607750024153\n",
      "Norm of gradient:  0.021323219448527043\n",
      "Cost function:  2.03019221649708\n",
      "Norm of gradient:  0.021249577192139598\n",
      "Cost function:  2.0301241324095423\n",
      "Norm of gradient:  0.021176535505036422\n",
      "Cost function:  2.030056517230191\n",
      "Norm of gradient:  0.021104086672329526\n",
      "Cost function:  2.029989365538822\n",
      "Norm of gradient:  0.021032223110267464\n",
      "Cost function:  2.029922672003196\n",
      "Norm of gradient:  0.020960937370119916\n",
      "Cost function:  2.02985643137708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of gradient:  0.020890222129268127\n",
      "Cost function:  2.0297906384986204\n",
      "Norm of gradient:  0.02082007019367477\n",
      "Cost function:  2.029725288288371\n",
      "Norm of gradient:  0.020750474490493713\n",
      "Cost function:  2.0296603757478167\n",
      "Norm of gradient:  0.02068142806954705\n",
      "Cost function:  2.0295958959574656\n",
      "Norm of gradient:  0.020612924097041248\n",
      "Cost function:  2.0295318440755574\n",
      "Norm of gradient:  0.020544955856334517\n",
      "Cost function:  2.0294682153362715\n",
      "Norm of gradient:  0.02047751674253695\n",
      "Cost function:  2.0294050050483063\n",
      "Norm of gradient:  0.020410600262701747\n",
      "Cost function:  2.029342208593129\n",
      "Norm of gradient:  0.020344200031204466\n",
      "Cost function:  2.0292798214240664\n",
      "Norm of gradient:  0.02027830976950946\n",
      "Cost function:  2.0292178390641755\n",
      "Norm of gradient:  0.020212923302129114\n",
      "Cost function:  2.029156257105564\n",
      "Norm of gradient:  0.02014803455602273\n",
      "Cost function:  2.0290950712073483\n",
      "Norm of gradient:  0.020083637557150504\n",
      "Cost function:  2.029034277095014\n",
      "Norm of gradient:  0.0200197264296078\n",
      "Cost function:  2.028973870558535\n",
      "Norm of gradient:  0.019956295392513757\n",
      "Cost function:  2.028913847451453\n",
      "Norm of gradient:  0.01989333875906134\n",
      "Cost function:  2.0288542036896295\n",
      "Norm of gradient:  0.019830850933720525\n",
      "Cost function:  2.028794935249822\n",
      "Norm of gradient:  0.019768826411156175\n",
      "Cost function:  2.028736038168887\n",
      "Norm of gradient:  0.019707259773694263\n",
      "Cost function:  2.0286775085421747\n",
      "Norm of gradient:  0.019646145690220166\n",
      "Cost function:  2.0286193425227834\n",
      "Norm of gradient:  0.019585478913851432\n",
      "Cost function:  2.028561536320332\n",
      "Norm of gradient:  0.01952525428076624\n",
      "Cost function:  2.0285040861999066\n",
      "Norm of gradient:  0.0194654667080818\n",
      "Cost function:  2.0284469884809764\n",
      "Norm of gradient:  0.01940611119269462\n",
      "Cost function:  2.0283902395362965\n",
      "Norm of gradient:  0.019347182809309145\n",
      "Cost function:  2.0283338357912992\n",
      "Norm of gradient:  0.019288676709250344\n",
      "Cost function:  2.028277773722521\n",
      "Norm of gradient:  0.019230588118679788\n",
      "Cost function:  2.028222049857232\n",
      "Norm of gradient:  0.01917291233734769\n",
      "Cost function:  2.0281666607720936\n",
      "Norm of gradient:  0.01911564473701722\n",
      "Cost function:  2.0281116030925452\n",
      "Norm of gradient:  0.01905878076020981\n",
      "Cost function:  2.0280568734915567\n",
      "Norm of gradient:  0.019002315918680224\n",
      "Cost function:  2.028002468689211\n",
      "Norm of gradient:  0.01894624579226128\n",
      "Cost function:  2.0279483854515186\n",
      "Norm of gradient:  0.01889056602739752\n",
      "Cost function:  2.027894620589708\n",
      "Norm of gradient:  0.018835272336029208\n",
      "Cost function:  2.027841170959314\n",
      "Norm of gradient:  0.01878036049424128\n",
      "Cost function:  2.027788033459617\n",
      "Norm of gradient:  0.01872582634108479\n",
      "Cost function:  2.0277352050326476\n",
      "Norm of gradient:  0.01867166577741938\n",
      "Cost function:  2.027682682662442\n",
      "Norm of gradient:  0.018617874764741024\n",
      "Cost function:  2.027630463374395\n",
      "Norm of gradient:  0.018564449324010645\n",
      "Cost function:  2.0275785442345375\n",
      "Norm of gradient:  0.01851138553461317\n",
      "Cost function:  2.0275269223487227\n",
      "Norm of gradient:  0.01845867953323458\n",
      "Cost function:  2.0274755948619543\n",
      "Norm of gradient:  0.018406327512810978\n",
      "Cost function:  2.0274245589577506\n",
      "Norm of gradient:  0.01835432572150646\n",
      "Cost function:  2.0273738118575975\n",
      "Norm of gradient:  0.01830267046169752\n",
      "Cost function:  2.027323350819893\n",
      "Norm of gradient:  0.018251358088970708\n",
      "Cost function:  2.027273173139778\n",
      "Norm of gradient:  0.018200385011170613\n",
      "Cost function:  2.02722327614826\n",
      "Norm of gradient:  0.018149747687466366\n",
      "Cost function:  2.02717365721161\n",
      "Norm of gradient:  0.018099442627379256\n",
      "Cost function:  2.027124313730762\n",
      "Norm of gradient:  0.0180494663899452\n",
      "Cost function:  2.02707524314083\n",
      "Norm of gradient:  0.017999815582772766\n",
      "Cost function:  2.027026442910481\n",
      "Norm of gradient:  0.01795048686119557\n",
      "Cost function:  2.0269779105412584\n",
      "Norm of gradient:  0.017901476927443684\n",
      "Cost function:  2.026929643567173\n",
      "Norm of gradient:  0.017852782529764827\n",
      "Cost function:  2.026881639554113\n",
      "Norm of gradient:  0.017804400461688574\n",
      "Cost function:  2.026833896099302\n",
      "Norm of gradient:  0.01775632756115008\n",
      "Cost function:  2.02678641083084\n",
      "Norm of gradient:  0.01770856070976669\n",
      "Cost function:  2.0267391814070557\n",
      "Norm of gradient:  0.017661096832049143\n",
      "Cost function:  2.0266922055161487\n",
      "Norm of gradient:  0.017613932894694744\n",
      "Cost function:  2.026645480875696\n",
      "Norm of gradient:  0.017567065905782674\n",
      "Cost function:  2.026599005232022\n",
      "Norm of gradient:  0.017520492914135922\n",
      "Cost function:  2.026552776359873\n",
      "Norm of gradient:  0.017474211008592554\n",
      "Cost function:  2.026506792062009\n",
      "Norm of gradient:  0.01742821731730718\n",
      "Cost function:  2.0264610501685034\n",
      "Norm of gradient:  0.017382509007101903\n",
      "Cost function:  2.0264155485366513\n",
      "Norm of gradient:  0.017337083282779917\n",
      "Cost function:  2.026370285050105\n",
      "Norm of gradient:  0.017291937386508215\n",
      "Cost function:  2.0263252576189243\n",
      "Norm of gradient:  0.017247068597209068\n",
      "Cost function:  2.0262804641788312\n",
      "Norm of gradient:  0.01720247422986547\n",
      "Cost function:  2.026235902690818\n",
      "Norm of gradient:  0.01715815163498692\n",
      "Cost function:  2.0261915711409477\n",
      "Norm of gradient:  0.017114098197989952\n",
      "Cost function:  2.0261474675397686\n",
      "Norm of gradient:  0.017070311338613715\n",
      "Cost function:  2.0261035899220237\n",
      "Norm of gradient:  0.017026788510342516\n",
      "Cost function:  2.0260599363462335\n",
      "Norm of gradient:  0.016983527199892115\n",
      "Cost function:  2.026016504894378\n",
      "Norm of gradient:  0.01694052492660594\n",
      "Cost function:  2.0259732936714414\n",
      "Norm of gradient:  0.016897779241952664\n",
      "Cost function:  2.0259303008051033\n",
      "Norm of gradient:  0.01685528772898938\n",
      "Cost function:  2.0258875244454746\n",
      "Norm of gradient:  0.016813048001834002\n",
      "Cost function:  2.0258449627645705\n",
      "Norm of gradient:  0.016771057705208564\n",
      "Cost function:  2.0258026139562246\n",
      "Norm of gradient:  0.016729314513878466\n",
      "Cost function:  2.025760476235405\n",
      "Norm of gradient:  0.016687816132210386\n",
      "Cost function:  2.0257185478382604\n",
      "Norm of gradient:  0.01664656029369344\n",
      "Cost function:  2.025676827021543\n",
      "Norm of gradient:  0.01660554476044819\n",
      "Cost function:  2.0256353120625046\n",
      "Norm of gradient:  0.01656476732279461\n",
      "Cost function:  2.0255940012583453\n",
      "Norm of gradient:  0.016524225798779543\n",
      "Cost function:  2.0255528929260542\n",
      "Norm of gradient:  0.016483918033769578\n",
      "Cost function:  2.0255119854022396\n",
      "Norm of gradient:  0.016443841899977423\n",
      "Cost function:  2.025471277042568\n",
      "Norm of gradient:  0.016403995296084783\n",
      "Cost function:  2.0254307662215987\n",
      "Norm of gradient:  0.016364376146777892\n",
      "Cost function:  2.025390451332667\n",
      "Norm of gradient:  0.016324982402371683\n",
      "Cost function:  2.0253503307872673\n",
      "Norm of gradient:  0.016285812038396644\n",
      "Cost function:  2.025310403015112\n",
      "Norm of gradient:  0.01624686305523955\n",
      "Cost function:  2.025270666463719\n",
      "Norm of gradient:  0.016208133477687688\n",
      "Cost function:  2.025231119598062\n",
      "Norm of gradient:  0.016169621354618316\n",
      "Cost function:  2.0251917609005377\n",
      "Norm of gradient:  0.01613132475859814\n",
      "Cost function:  2.0251525888705313\n",
      "Norm of gradient:  0.016093241785518294\n",
      "Cost function:  2.0251136020242058\n",
      "Norm of gradient:  0.016055370554269154\n",
      "Cost function:  2.025074798894395\n",
      "Norm of gradient:  0.016017709206319248\n",
      "Cost function:  2.025036178030111\n",
      "Norm of gradient:  0.01598025590544754\n",
      "Cost function:  2.0249977379965727\n",
      "Norm of gradient:  0.015943008837360727\n",
      "Cost function:  2.02495947737481\n",
      "Norm of gradient:  0.015905966209381797\n",
      "Cost function:  2.024921394761454\n",
      "Norm of gradient:  0.01586912625010335\n",
      "Cost function:  2.0248834887686495\n",
      "Norm of gradient:  0.015832487209097912\n",
      "Cost function:  2.024845758023604\n",
      "Norm of gradient:  0.01579604735657681\n",
      "Cost function:  2.0248082011686064\n",
      "Norm of gradient:  0.015759804983087915\n",
      "Cost function:  2.024770816860739\n",
      "Norm of gradient:  0.01572375839922927\n",
      "Cost function:  2.0247336037715025\n",
      "Norm of gradient:  0.015687905935338753\n",
      "Cost function:  2.024696560586883\n",
      "Norm of gradient:  0.01565224594119453\n",
      "Cost function:  2.0246596860070065\n",
      "Norm of gradient:  0.015616776785730242\n",
      "Cost function:  2.0246229787459566\n",
      "Norm of gradient:  0.015581496856790524\n",
      "Cost function:  2.0245864375315485\n",
      "Norm of gradient:  0.015546404560784889\n",
      "Cost function:  2.02455006110519\n",
      "Norm of gradient:  0.015511498322496337\n",
      "Cost function:  2.0245138482217877\n",
      "Norm of gradient:  0.015476776584741154\n",
      "Cost function:  2.024477797649302\n",
      "Norm of gradient:  0.015442237808151833\n",
      "Cost function:  2.0244419081688187\n",
      "Norm of gradient:  0.015407880470901972\n",
      "Cost function:  2.0244061785742806\n",
      "Norm of gradient:  0.015373703068459784\n",
      "Cost function:  2.0243706076721955\n",
      "Norm of gradient:  0.015339704113343325\n",
      "Cost function:  2.024335194281777\n",
      "Norm of gradient:  0.015305882134851775\n",
      "Cost function:  2.0242999372343835\n",
      "Norm of gradient:  0.015272235678875751\n",
      "Cost function:  2.0242648353736605\n",
      "Norm of gradient:  0.015238763307593\n",
      "Cost function:  2.024229887555245\n",
      "Norm of gradient:  0.0152054635992964\n",
      "Cost function:  2.024195092646571\n",
      "Norm of gradient:  0.015172335148116804\n",
      "Cost function:  2.0241604495267977\n",
      "Norm of gradient:  0.015139376563866573\n",
      "Cost function:  2.0241259570866736\n",
      "Norm of gradient:  0.015106586471735288\n",
      "Cost function:  2.024091614228234\n",
      "Norm of gradient:  0.015073963512119844\n",
      "Cost function:  2.024057419864767\n",
      "Norm of gradient:  0.015041506340448148\n",
      "Cost function:  2.0240233729207104\n",
      "Norm of gradient:  0.015009213626865518\n",
      "Cost function:  2.0239894723313596\n",
      "Norm of gradient:  0.014977084056145139\n",
      "Cost function:  2.0239557170428784\n",
      "Norm of gradient:  0.014945116327391804\n",
      "Cost function:  2.0239221060120425\n",
      "Norm of gradient:  0.014913309153916336\n",
      "Cost function:  2.0238886382061096\n",
      "Norm of gradient:  0.01488166126298199\n",
      "Cost function:  2.023855312602798\n",
      "Norm of gradient:  0.014850171395659839\n",
      "Cost function:  2.0238221281900444\n",
      "Norm of gradient:  0.014818838306593524\n",
      "Cost function:  2.023789083965812\n",
      "Norm of gradient:  0.01478766076384457\n",
      "Cost function:  2.0237561789381964\n",
      "Norm of gradient:  0.014756637548724966\n",
      "Cost function:  2.0237234121250274\n",
      "Norm of gradient:  0.014725767455565707\n",
      "Cost function:  2.0236907825539223\n",
      "Norm of gradient:  0.014695049291567293\n",
      "Cost function:  2.0236582892620496\n",
      "Norm of gradient:  0.014664481876658335\n",
      "Cost function:  2.023625931296147\n",
      "Norm of gradient:  0.01463406404325628\n",
      "Cost function:  2.0235937077122266\n",
      "Norm of gradient:  0.014603794636153078\n",
      "Cost function:  2.0235616175755955\n",
      "Norm of gradient:  0.014573672512334018\n",
      "Cost function:  2.0235296599606882\n",
      "Norm of gradient:  0.014543696540810641\n",
      "Cost function:  2.0234978339509215\n",
      "Norm of gradient:  0.014513865602444437\n",
      "Cost function:  2.023466138638604\n",
      "Norm of gradient:  0.014484178589846389\n",
      "Cost function:  2.0234345731249257\n",
      "Norm of gradient:  0.014454634407146017\n",
      "Cost function:  2.0234031365195957\n",
      "Norm of gradient:  0.014425231969893497\n",
      "Cost function:  2.0233718279410384\n",
      "Norm of gradient:  0.014395970204884475\n",
      "Cost function:  2.0233406465160955\n",
      "Norm of gradient:  0.0143668480500134\n",
      "Cost function:  2.023309591379938\n",
      "Norm of gradient:  0.014337864454158019\n",
      "Cost function:  2.0232786616760414\n",
      "Norm of gradient:  0.014309018376978854\n",
      "Cost function:  2.023247856555977\n",
      "Norm of gradient:  0.014280308788824267\n",
      "Cost function:  2.0232171751794086\n",
      "Norm of gradient:  0.014251734670591812\n",
      "Cost function:  2.023186616713975\n",
      "Norm of gradient:  0.014223295013552744\n",
      "Cost function:  2.0231561803351386\n",
      "Norm of gradient:  0.014194988819259466\n",
      "Cost function:  2.0231258652261728\n",
      "Norm of gradient:  0.014166815099395472\n",
      "Cost function:  2.023095670577969\n",
      "Norm of gradient:  0.014138772875641506\n",
      "Cost function:  2.023065595589016\n",
      "Norm of gradient:  0.01411086117955704\n",
      "Cost function:  2.0230356394652915\n",
      "Norm of gradient:  0.014083079052440092\n",
      "Cost function:  2.0230058014201573\n",
      "Norm of gradient:  0.014055425545222008\n",
      "Cost function:  2.0229760806743196\n",
      "Norm of gradient:  0.014027899718331674\n",
      "Cost function:  2.0229464764556466\n",
      "Norm of gradient:  0.01400050064156567\n",
      "Cost function:  2.0229169879991895\n",
      "Norm of gradient:  0.01397322739400937\n",
      "Cost function:  2.0228876145470496\n",
      "Norm of gradient:  0.013946079063867526\n",
      "Cost function:  2.0228583553482444\n",
      "Norm of gradient:  0.013919054748382382\n",
      "Cost function:  2.022829209658705\n",
      "Norm of gradient:  0.013892153553716565\n",
      "Cost function:  2.0228001767412014\n",
      "Norm of gradient:  0.013865374594838026\n",
      "Cost function:  2.022771255865143\n",
      "Norm of gradient:  0.013838716995387192\n",
      "Cost function:  2.022742446306636\n",
      "Norm of gradient:  0.013812179887638837\n",
      "Cost function:  2.02271374734836\n",
      "Norm of gradient:  0.013785762412298402\n",
      "Cost function:  2.022685158279466\n",
      "Norm of gradient:  0.013759463718467336\n",
      "Cost function:  2.022656678395478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of gradient:  0.013733282963517914\n",
      "Cost function:  2.0226283069983433\n",
      "Norm of gradient:  0.013707219312999449\n",
      "Cost function:  2.0226000433962\n",
      "Norm of gradient:  0.013681271940516816\n",
      "Cost function:  2.02257188690342\n",
      "Norm of gradient:  0.013655440027649252\n",
      "Cost function:  2.022543836840473\n",
      "Norm of gradient:  0.013629722763847543\n",
      "Cost function:  2.0225158925339137\n",
      "Norm of gradient:  0.013604119346344892\n",
      "Cost function:  2.0224880533162093\n",
      "Norm of gradient:  0.01357862898005645\n",
      "Cost function:  2.02246031852582\n",
      "Norm of gradient:  0.013553250877481706\n",
      "Cost function:  2.0224326875069822\n",
      "Norm of gradient:  0.013527984258618985\n",
      "Cost function:  2.022405159609802\n",
      "Norm of gradient:  0.013502828350872695\n",
      "Cost function:  2.0223777341899645\n",
      "Norm of gradient:  0.013477782388964767\n",
      "Cost function:  2.022350410608948\n",
      "Norm of gradient:  0.013452845614848264\n",
      "Cost function:  2.022323188233723\n",
      "Norm of gradient:  0.013428017277626351\n",
      "Cost function:  2.022296066436811\n",
      "Norm of gradient:  0.013403296633437473\n",
      "Cost function:  2.022269044596167\n",
      "Norm of gradient:  0.013378682945428282\n",
      "Cost function:  2.0222421220951983\n",
      "Norm of gradient:  0.013354175483601429\n",
      "Cost function:  2.0222152983226094\n",
      "Norm of gradient:  0.01332977352479774\n",
      "Cost function:  2.0221885726724045\n",
      "Norm of gradient:  0.0133054763525565\n",
      "Cost function:  2.0221619445437815\n",
      "Norm of gradient:  0.013281283257098197\n",
      "Cost function:  2.0221354133410996\n",
      "Norm of gradient:  0.013257193535195583\n",
      "Cost function:  2.0221089784738995\n",
      "Norm of gradient:  0.013233206490123803\n",
      "Cost function:  2.022082639356662\n",
      "Norm of gradient:  0.013209321431559116\n",
      "Cost function:  2.022056395408936\n",
      "Norm of gradient:  0.013185537675543368\n",
      "Cost function:  2.022030246055174\n",
      "Norm of gradient:  0.013161854544371182\n",
      "Cost function:  2.022004190724741\n",
      "Norm of gradient:  0.0131382713665554\n",
      "Cost function:  2.021978228851783\n",
      "Norm of gradient:  0.013114787476697705\n",
      "Cost function:  2.021952359875298\n",
      "Norm of gradient:  0.013091402215484988\n",
      "Cost function:  2.021926583238941\n",
      "Norm of gradient:  0.01306811492957043\n",
      "Cost function:  2.021900898391097\n",
      "Norm of gradient:  0.013044924971533325\n",
      "Cost function:  2.0218753047847193\n",
      "Norm of gradient:  0.013021831699790218\n",
      "Cost function:  2.021849801877392\n",
      "Norm of gradient:  0.01299883447854914\n",
      "Cost function:  2.0218243891311865\n",
      "Norm of gradient:  0.012975932677717952\n",
      "Cost function:  2.0217990660126537\n",
      "Norm of gradient:  0.012953125672865535\n",
      "Cost function:  2.021773831992814\n",
      "Norm of gradient:  0.012930412845146205\n",
      "Cost function:  2.02174868654702\n",
      "Norm of gradient:  0.01290779358121573\n",
      "Cost function:  2.0217236291549767\n",
      "Norm of gradient:  0.012885267273220622\n",
      "Cost function:  2.021698659300667\n",
      "Norm of gradient:  0.012862833318682335\n",
      "Cost function:  2.021673776472376\n",
      "Norm of gradient:  0.01284049112045591\n",
      "Cost function:  2.0216489801624773\n",
      "Norm of gradient:  0.012818240086693486\n",
      "Cost function:  2.0216242698676092\n",
      "Norm of gradient:  0.012796079630749501\n",
      "Cost function:  2.021599645088449\n",
      "Norm of gradient:  0.012774009171134863\n",
      "Cost function:  2.0215751053297666\n",
      "Norm of gradient:  0.012752028131480913\n",
      "Cost function:  2.0215506501003695\n",
      "Norm of gradient:  0.0127301359404427\n",
      "Cost function:  2.021526278913008\n",
      "Norm of gradient:  0.012708332031679608\n",
      "Cost function:  2.021501991284397\n",
      "Norm of gradient:  0.01268661584377479\n",
      "Cost function:  2.0214777867351668\n",
      "Norm of gradient:  0.012664986820195123\n",
      "Cost function:  2.02145366478981\n",
      "Norm of gradient:  0.0126434444092302\n",
      "Cost function:  2.0214296249765775\n",
      "Norm of gradient:  0.012621988063952056\n",
      "Cost function:  2.021405666827554\n",
      "Norm of gradient:  0.012600617242132682\n",
      "Cost function:  2.021381789878553\n",
      "Norm of gradient:  0.012579331406230668\n",
      "Cost function:  2.021357993669091\n",
      "Norm of gradient:  0.012558130023320388\n",
      "Cost function:  2.0213342777423655\n",
      "Norm of gradient:  0.012537012565032027\n",
      "Cost function:  2.0213106416451643\n",
      "Norm of gradient:  0.012515978507531134\n",
      "Cost function:  2.0212870849278435\n",
      "Norm of gradient:  0.012495027331421599\n",
      "Cost function:  2.021263607144424\n",
      "Norm of gradient:  0.01247415852176961\n",
      "Cost function:  2.021240207852326\n",
      "Norm of gradient:  0.012453371567973019\n",
      "Cost function:  2.0212168866124935\n",
      "Norm of gradient:  0.01243266596379092\n",
      "Cost function:  2.0211936429893536\n",
      "Norm of gradient:  0.01241204120723097\n",
      "Cost function:  2.0211704765506537\n",
      "Norm of gradient:  0.01239149680055661\n",
      "Cost function:  2.0211473868676286\n",
      "Norm of gradient:  0.012371032250194609\n",
      "Cost function:  2.021124373514743\n",
      "Norm of gradient:  0.012350647066743799\n",
      "Cost function:  2.0211014360698725\n",
      "Norm of gradient:  0.01233034076488209\n",
      "Cost function:  2.021078574114104\n",
      "Norm of gradient:  0.012310112863353784\n",
      "Cost function:  2.0210557872318082\n",
      "Norm of gradient:  0.012289962884904448\n",
      "Cost function:  2.0210330750104974\n",
      "Norm of gradient:  0.012269890356262855\n",
      "Cost function:  2.0210104370409745\n",
      "Norm of gradient:  0.012249894808074903\n",
      "Cost function:  2.0209878729170656\n",
      "Norm of gradient:  0.012229975774879042\n",
      "Cost function:  2.0209653822358185\n",
      "Norm of gradient:  0.012210132795064594\n",
      "Cost function:  2.020942964597295\n",
      "Norm of gradient:  0.012190365410809584\n",
      "Cost function:  2.020920619604666\n",
      "Norm of gradient:  0.01217067316808067\n",
      "Cost function:  2.0208983468640613\n",
      "Norm of gradient:  0.012151055616560102\n",
      "Cost function:  2.0208761459846865\n",
      "Norm of gradient:  0.01213151230960133\n",
      "Cost function:  2.0208540165786504\n",
      "Norm of gradient:  0.012112042804243188\n",
      "Cost function:  2.020831958261012\n",
      "Norm of gradient:  0.012092646661102605\n",
      "Cost function:  2.0208099706497675\n",
      "Norm of gradient:  0.012073323444394544\n",
      "Cost function:  2.0207880533657665\n",
      "Norm of gradient:  0.012054072721862092\n",
      "Cost function:  2.02076620603271\n",
      "Norm of gradient:  0.01203489406474897\n",
      "Cost function:  2.0207444282771103\n",
      "Norm of gradient:  0.012015787047769356\n",
      "Cost function:  2.0207227197283415\n",
      "Norm of gradient:  0.011996751249063665\n",
      "Cost function:  2.0207010800184566\n",
      "Norm of gradient:  0.01197778625014775\n",
      "Cost function:  2.0206795087823366\n",
      "Norm of gradient:  0.011958891635934298\n",
      "Cost function:  2.020658005657544\n",
      "Norm of gradient:  0.01194006699463137\n",
      "Cost function:  2.0206365702842994\n",
      "Norm of gradient:  0.011921311917766308\n",
      "Cost function:  2.0206152023055504\n",
      "Norm of gradient:  0.011902626000101245\n",
      "Cost function:  2.020593901366828\n",
      "Norm of gradient:  0.011884008839625648\n",
      "Cost function:  2.020572667116324\n",
      "Norm of gradient:  0.011865460037540363\n",
      "Cost function:  2.020551499204793\n",
      "Norm of gradient:  0.01184697919819101\n",
      "Cost function:  2.0205303972855413\n",
      "Norm of gradient:  0.011828565929061435\n",
      "Cost function:  2.0205093610144864\n",
      "Norm of gradient:  0.011810219840722284\n",
      "Cost function:  2.0204883900499846\n",
      "Norm of gradient:  0.011791940546815594\n",
      "Cost function:  2.0204674840529186\n",
      "Norm of gradient:  0.011773727664028288\n",
      "Cost function:  2.0204466426866423\n",
      "Norm of gradient:  0.011755580812045132\n",
      "Cost function:  2.0204258656169523\n",
      "Norm of gradient:  0.011737499613527111\n",
      "Cost function:  2.020405152512072\n",
      "Norm of gradient:  0.011719483694076861\n",
      "Cost function:  2.020384503042612\n",
      "Norm of gradient:  0.011701532682230013\n",
      "Cost function:  2.0203639168815912\n",
      "Norm of gradient:  0.011683646209392749\n",
      "Cost function:  2.0203433937043784\n",
      "Norm of gradient:  0.011665823909841664\n",
      "Cost function:  2.0203229331886545\n",
      "Norm of gradient:  0.01164806542068416\n",
      "Cost function:  2.0203025350144\n",
      "Norm of gradient:  0.011630370381829543\n",
      "Cost function:  2.0202821988639608\n",
      "Norm of gradient:  0.01161273843596432\n",
      "Cost function:  2.0202619244218862\n",
      "Norm of gradient:  0.011595169228530185\n",
      "Cost function:  2.0202417113749966\n",
      "Norm of gradient:  0.011577662407667649\n",
      "Cost function:  2.0202215594123505\n",
      "Norm of gradient:  0.011560217624244827\n",
      "Cost function:  2.020201468225215\n",
      "Norm of gradient:  0.011542834531784606\n",
      "Cost function:  2.0201814375070177\n",
      "Norm of gradient:  0.011525512786456298\n",
      "Cost function:  2.0201614669533954\n",
      "Norm of gradient:  0.011508252047037903\n",
      "Cost function:  2.0201415562621\n",
      "Norm of gradient:  0.0114910519749231\n",
      "Cost function:  2.0201217051330307\n",
      "Norm of gradient:  0.011473912234047305\n",
      "Cost function:  2.0201019132681868\n",
      "Norm of gradient:  0.011456832490902858\n",
      "Cost function:  2.0200821803716806\n",
      "Norm of gradient:  0.011439812414515178\n",
      "Cost function:  2.0200625061496833\n",
      "Norm of gradient:  0.011422851676360455\n",
      "Cost function:  2.020042890310415\n",
      "Norm of gradient:  0.011405949950439163\n",
      "Cost function:  2.0200233325641106\n",
      "Norm of gradient:  0.011389106913166363\n",
      "Cost function:  2.0200038326230914\n",
      "Norm of gradient:  0.011372322243383772\n",
      "Cost function:  2.019984390201575\n",
      "Norm of gradient:  0.01135559562234764\n",
      "Cost function:  2.01996500501586\n",
      "Norm of gradient:  0.011338926733682194\n",
      "Cost function:  2.0199456767841486\n",
      "Norm of gradient:  0.01132231526336241\n",
      "Cost function:  2.019926405226614\n",
      "Norm of gradient:  0.011305760899727624\n",
      "Cost function:  2.0199071900653434\n",
      "Norm of gradient:  0.011289263333373207\n",
      "Cost function:  2.0198880310243146\n",
      "Norm of gradient:  0.011272822257245223\n",
      "Cost function:  2.0198689278294575\n",
      "Norm of gradient:  0.011256437366504028\n",
      "Cost function:  2.019849880208552\n",
      "Norm of gradient:  0.011240108358607392\n",
      "Cost function:  2.0198308878912163\n",
      "Norm of gradient:  0.011223834933203591\n",
      "Cost function:  2.0198119506089274\n",
      "Norm of gradient:  0.011207616792161233\n",
      "Cost function:  2.019793068095006\n",
      "Norm of gradient:  0.011191453639511942\n",
      "Cost function:  2.019774240084574\n",
      "Norm of gradient:  0.011175345181482653\n",
      "Cost function:  2.0197554663145545\n",
      "Norm of gradient:  0.011159291126438868\n",
      "Cost function:  2.0197367465236296\n",
      "Norm of gradient:  0.011143291184851292\n",
      "Cost function:  2.0197180804522943\n",
      "Norm of gradient:  0.011127345069311635\n",
      "Cost function:  2.019699467842734\n",
      "Norm of gradient:  0.011111452494488095\n",
      "Cost function:  2.0196809084389145\n",
      "Norm of gradient:  0.011095613177122918\n",
      "Cost function:  2.0196624019865226\n",
      "Norm of gradient:  0.01107982683601763\n",
      "Cost function:  2.0196439482328925\n",
      "Norm of gradient:  0.011064093191958942\n",
      "Cost function:  2.0196255469271263\n",
      "Norm of gradient:  0.011048411967792555\n",
      "Cost function:  2.0196071978199415\n",
      "Norm of gradient:  0.011032782888329758\n",
      "Cost function:  2.0195889006637535\n",
      "Norm of gradient:  0.011017205680353057\n",
      "Cost function:  2.019570655212605\n",
      "Norm of gradient:  0.011001680072631003\n",
      "Cost function:  2.0195524612221787\n",
      "Norm of gradient:  0.010986205795820337\n",
      "Cost function:  2.0195343184497894\n",
      "Norm of gradient:  0.010970782582539845\n",
      "Cost function:  2.0195162266543285\n",
      "Norm of gradient:  0.010955410167294182\n",
      "Cost function:  2.019498185596312\n",
      "Norm of gradient:  0.010940088286481704\n",
      "Cost function:  2.0194801950377936\n",
      "Norm of gradient:  0.01092481667834217\n",
      "Cost function:  2.019462254742421\n",
      "Norm of gradient:  0.010909595083019745\n",
      "Cost function:  2.0194443644753948\n",
      "Norm of gradient:  0.01089442324243864\n",
      "Cost function:  2.0194265240034492\n",
      "Norm of gradient:  0.010879300900380983\n",
      "Cost function:  2.019408733094826\n",
      "Norm of gradient:  0.010864227802400258\n",
      "Cost function:  2.01939099151932\n",
      "Norm of gradient:  0.01084920369587243\n",
      "Cost function:  2.019373299048172\n",
      "Norm of gradient:  0.010834228329917678\n",
      "Cost function:  2.019355655454129\n",
      "Norm of gradient:  0.010819301455412562\n",
      "Cost function:  2.0193380605114393\n",
      "Norm of gradient:  0.010804422824981846\n",
      "Cost function:  2.019320513995814\n",
      "Norm of gradient:  0.010789592192970439\n",
      "Cost function:  2.0193030156843745\n",
      "Norm of gradient:  0.010774809315431769\n",
      "Cost function:  2.019285565355708\n",
      "Norm of gradient:  0.010760073950117204\n",
      "Cost function:  2.01926816278981\n",
      "Norm of gradient:  0.010745385856448542\n",
      "Cost function:  2.019250807768114\n",
      "Norm of gradient:  0.010730744795521018\n",
      "Cost function:  2.019233500073428\n",
      "Norm of gradient:  0.01071615053007222\n",
      "Cost function:  2.0192162394899937\n",
      "Norm of gradient:  0.01070160282447289\n",
      "Cost function:  2.0191990258033963\n",
      "Norm of gradient:  0.010687101444730384\n",
      "Cost function:  2.01918185880058\n",
      "Norm of gradient:  0.010672646158419544\n",
      "Cost function:  2.019164738269911\n",
      "Norm of gradient:  0.010658236734760712\n",
      "Cost function:  2.019147664001016\n",
      "Norm of gradient:  0.010643872944523602\n",
      "Cost function:  2.019130635784903\n",
      "Norm of gradient:  0.010629554560041797\n",
      "Cost function:  2.0191136534138936\n",
      "Norm of gradient:  0.010615281355193209\n",
      "Cost function:  2.0190967166816467\n",
      "Norm of gradient:  0.010601053105426172\n",
      "Cost function:  2.019079825383094\n",
      "Norm of gradient:  0.010586869587671618\n",
      "Cost function:  2.0190629793144628\n",
      "Norm of gradient:  0.01057273058040718\n",
      "Cost function:  2.019046178273263\n",
      "Norm of gradient:  0.010558635863590437\n",
      "Cost function:  2.0190294220582907\n",
      "Norm of gradient:  0.010544585218669104\n",
      "Cost function:  2.0190127104696094\n",
      "Norm of gradient:  0.010530578428573797\n",
      "Cost function:  2.018996043308488\n",
      "Norm of gradient:  0.010516615277668562\n",
      "Cost function:  2.01897942037748\n",
      "Norm of gradient:  0.01050269555179209\n",
      "Cost function:  2.018962841480339\n",
      "Norm of gradient:  0.010488819038208277\n",
      "Cost function:  2.0189463064220776\n",
      "Norm of gradient:  0.010474985525612653\n",
      "Cost function:  2.0189298150088746\n",
      "Norm of gradient:  0.010461194804096003\n",
      "Cost function:  2.018913367048124\n",
      "Norm of gradient:  0.01044744666516113\n",
      "Cost function:  2.0188969623484585\n",
      "Norm of gradient:  0.010433740901697184\n",
      "Cost function:  2.0188806007196147\n",
      "Norm of gradient:  0.01042007730795662\n",
      "Cost function:  2.0188642819725544\n",
      "Norm of gradient:  0.01040645567958774\n",
      "Cost function:  2.0188480059194016\n",
      "Norm of gradient:  0.010392875813551337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost function:  2.0188317723734213\n",
      "Norm of gradient:  0.010379337508172078\n",
      "Cost function:  2.0188155811490045\n",
      "Norm of gradient:  0.010365840563111236\n",
      "Cost function:  2.018799432061707\n",
      "Norm of gradient:  0.01035238477932995\n",
      "Cost function:  2.0187833249282177\n",
      "Norm of gradient:  0.010338969959108452\n",
      "Cost function:  2.01876725956631\n",
      "Norm of gradient:  0.010325595906028372\n",
      "Cost function:  2.0187512357948982\n",
      "Norm of gradient:  0.010312262424952095\n",
      "Cost function:  2.0187352534339826\n",
      "Norm of gradient:  0.01029896932200846\n",
      "Cost function:  2.018719312304664\n",
      "Norm of gradient:  0.010285716404605048\n",
      "Cost function:  2.0187034122291143\n",
      "Norm of gradient:  0.01027250348140255\n",
      "Cost function:  2.0186875530305883\n",
      "Norm of gradient:  0.01025933036231177\n",
      "Cost function:  2.018671734533423\n",
      "Norm of gradient:  0.010246196858448867\n",
      "Cost function:  2.018655956562972\n",
      "Norm of gradient:  0.010233102782187985\n",
      "Cost function:  2.0186402189456985\n",
      "Norm of gradient:  0.010220047947106565\n",
      "Cost function:  2.0186245215090546\n",
      "Norm of gradient:  0.010207032167970262\n",
      "Cost function:  2.018608864081516\n",
      "Norm of gradient:  0.010194055260751279\n",
      "Cost function:  2.0185932464927077\n",
      "Norm of gradient:  0.010181117042620588\n",
      "Cost function:  2.0185776685730907\n",
      "Norm of gradient:  0.010168217331893265\n",
      "Cost function:  2.0185621301542715\n",
      "Norm of gradient:  0.01015535594807621\n",
      "Cost function:  2.0185466310688\n",
      "Norm of gradient:  0.010142532711814608\n",
      "Cost function:  2.018531171150242\n",
      "Norm of gradient:  0.010129747444897768\n",
      "Cost function:  2.0185157502331563\n",
      "Norm of gradient:  0.010116999970283076\n",
      "Cost function:  2.0185003681530596\n",
      "Norm of gradient:  0.010104290112019982\n",
      "Cost function:  2.018485024746431\n",
      "Norm of gradient:  0.010091617695276974\n",
      "Cost function:  2.0184697198507795\n",
      "Norm of gradient:  0.010078982546357983\n",
      "Cost function:  2.0184544533044897\n",
      "Norm of gradient:  0.010066384492655489\n",
      "Cost function:  2.018439224946974\n",
      "Norm of gradient:  0.010053823362647046\n",
      "Cost function:  2.018424034618488\n",
      "Norm of gradient:  0.010041298985886887\n",
      "Cost function:  2.0184088821603554\n",
      "Norm of gradient:  0.010028811193035338\n",
      "Cost function:  2.0183937674146986\n",
      "Norm of gradient:  0.01001635981578578\n",
      "Cost function:  2.018378690224665\n",
      "Norm of gradient:  0.010003944686907842\n",
      "Cost function:  2.0183636504342424\n",
      "Norm of gradient:  0.009991565640206864\n",
      "Cost function:  2.0183486478883808\n",
      "Norm of gradient:  0.009979222510542498\n",
      "Cost function:  2.0183336824328997\n",
      "Norm of gradient:  0.00996691513378779\n",
      "Cost function:  2.0183187539145204\n",
      "Norm of gradient:  0.0099546433468634\n",
      "Cost function:  2.01830386218085\n",
      "Norm of gradient:  0.009942406987686713\n",
      "Cost function:  2.01828900708039\n",
      "Norm of gradient:  0.009930205895198342\n",
      "Cost function:  2.0182741884625184\n",
      "Norm of gradient:  0.00991803990931196\n",
      "Cost function:  2.018259406177443\n",
      "Norm of gradient:  0.009905908870976378\n",
      "Cost function:  2.0182446600762822\n",
      "Norm of gradient:  0.009893812622091411\n",
      "Cost function:  2.0182299500109853\n",
      "Norm of gradient:  0.009881751005528811\n",
      "Cost function:  2.018215275834354\n",
      "Norm of gradient:  0.009869723865160916\n",
      "Cost function:  2.0182006374000543\n",
      "Norm of gradient:  0.009857731045782504\n",
      "Cost function:  2.018186034562533\n",
      "Norm of gradient:  0.009845772393188543\n",
      "Cost function:  2.0181714671771207\n",
      "Norm of gradient:  0.009833847754086357\n",
      "Cost function:  2.0181569350999613\n",
      "Norm of gradient:  0.009821956976116836\n",
      "Cost function:  2.018142438188013\n",
      "Norm of gradient:  0.009810099907886451\n",
      "Cost function:  2.0181279762990196\n",
      "Norm of gradient:  0.009798276398893004\n",
      "Cost function:  2.018113549291582\n",
      "Norm of gradient:  0.009786486299570837\n",
      "Cost function:  2.018099157025063\n",
      "Norm of gradient:  0.009774729461256862\n",
      "Cost function:  2.0180847993596407\n"
     ]
    }
   ],
   "source": [
    "# Reduced gradient iteration\n",
    "\n",
    "max_iter = 600\n",
    "xk = np.copy(x)\n",
    "uk = np.copy(u)\n",
    "\n",
    "for i in range(max_iter):\n",
    "    \n",
    "    # power flow\n",
    "    xk = powerflow(xk, uk, p)\n",
    "\n",
    "    # lambda calculation\n",
    "    J_x = gfun_x(xk, uk, p)\n",
    "    G_x = cfun_x(xk, uk, p)\n",
    "\n",
    "    lam = -np.dot(inv(np.transpose(J_x)), G_x)\n",
    "    \n",
    "    # gradient cost function\n",
    "    J_u = gfun_u(xk, uk, p)\n",
    "    G_u = cfun_u(xk, uk, p)\n",
    "    \n",
    "    grad_c = G_u + np.dot(np.transpose(J_u), lam)\n",
    "    print(\"Norm of gradient: \", np.linalg.norm(grad_c))\n",
    "    \n",
    "    # evaluate cost function\n",
    "    print(\"Cost function: \", cfun(xk, uk, p))    \n",
    "    \n",
    "    # compute step\n",
    "    alpha = 0.15\n",
    "    uk = uk - alpha*grad_c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
